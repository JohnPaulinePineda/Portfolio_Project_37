---
title: 'Unsupervised Learning : Uncovering Underlying Constructs of Chronic Disease Indicators Across US States Using Exploratory and Confirmatory Factor Analyses'
author: "John Pauline Pineda"
date: "November 5, 2023"
output: 
  html_document:
    toc: true
    toc_depth: 4
    theme: readable
    highlight: tango
    css: doc.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=15, fig.height=10)
```
# **1. Table of Contents**
|
##  1.1 Introduction
|
| Chronic disease indicators (CDI) are a set of surveillance indicators developed by consensus among the [Center for Disease Controls and Prevention (CDC)](https://www.cdc.gov/), [Council of State and Territorial Epidemiologists (CSTE)](https://www.cste.org/), and [National Association of Chronic Disease Directors (NACDD)](https://chronicdisease.org/). [CDI enables public health professionals and policymakers to retrieve uniformly defined state-level data for chronic diseases and risk factors](https://www.cdc.gov/cdi/overview.html) that have a substantial impact on public health. These indicators are essential for surveillance, prioritization, and evaluation of public health interventions.
|
| Using an open dataset from [Data.World](https://data.world/) as primarily sourced from [CDC](https://www.cdc.gov/cdi/index.html), this study hypothesized that latent patterns are present among sufficiently correlated [indicators](https://datatopics.worldbank.org/world-development-indicators/) encompassing multiple chronic disease topic areas between the different US states. A number of factor analysis models was formulated to explore and verify the relationship between these factors.
|
| Subsequent analysis and modelling steps involving data understanding, data preparation, data exploration, model development, model validation and model presentation were individually detailed below, with all the results consolidated in a [<span style="color: #FF0000">**Summary**</span>](#summary) provided at the end of the document.
|
###  1.1.1 Study Objectives
|
| **The main objective of the study is to explore and validate potential underlying factor structures and relationships among a set of observed US chronic disease indicators by identifying latent factors that explain the observed correlations and reducing the complexity of the data by grouping related indicators together under these discovered factors.**
|
| Specific objectives are given as follows:
|
| **[A]** Obtain an optimal subset of observations and descriptors by conducting data quality assessment and applying preprocessing operations most suitable for the downstream analysis
|
| **[B]** Develop exploratory factor analysis models using various extraction methods to estimate and identify potential underlying structures from observed descriptors and different rotation approaches in simplifying the derived factor structures to achieve a more interpretable pattern of factor loadings
|
| **[C]** Select the final exploratory factor analysis model among candidates based on robust performance estimates
|
| **[D]** Validate the relationship obtained from the final exploratory analysis model between the observed descriptors and their underlying latent constructs using confirmatory factor analysis
|
###  1.1.2 Outcome
|
| There is no explicit endpoint for the study given the unsupervised learning nature of the analysis.
|
###  1.1.3 Descriptors
|
| Detailed information for each individual descriptor used in the study are provided as follows:
|
| **[A]** <span style="color: #FF0000">LDMORT</span> (numeric): **Liver Disease Mortality**; Chronic liver disease mortality.
|
| **[B]** <span style="color: #FF0000">ARTINC</span> (numeric): **Arthritis Incidence**; Arthritis among adults aged >= 18 years.
|
| **[C]** <span style="color: #FF0000">ASMORT</span> (numeric): **Asthma Mortality**; Asthma mortality rate.
|
| **[D]** <span style="color: #FF0000">PSMUSE</span> (numeric): **Papanicolaou Smear Use**; Papanicolaou smear use among adult women aged 21-65 years.
|
| **[E]** <span style="color: #FF0000">RDMORT</span> (numeric): **Renal Disease Mortality**; Mortality with end-stage renal disease.
|
| **[F]** <span style="color: #FF0000">PDMORT</span> (numeric): **Pulmonary Disease Mortality**; Mortality with chronic obstructive pulmonary disease as underlying cause among adults aged >=45 years.
|
| **[G]** <span style="color: #FF0000">CDMORT</span> (numeric): **Cardiovascular Disease Mortality**; Mortality from total cardiovascular disease.
|
| **[H]** <span style="color: #FF0000">HDMORT</span> (numeric): **Heart Disease Mortality**; Mortality from diseases of the heart.
|
| **[I]** <span style="color: #FF0000">STMORT</span> (numeric): **Stroke Mortality**; Mortality from stroke.
|
| **[J]** <span style="color: #FF0000">DBMORT</span> (numeric): **Diabetes Mortality**; Mortality due to diabetes reported as any listed cause of death.
|
| **[K]** <span style="color: #FF0000">INFVAC</span> (numeric): **Influenza Vaccination**; Influenza vaccination among noninstitutionalized adults aged >= 18 years.
|
| **[L]** <span style="color: #FF0000">MEUNDA</span> (numeric): **Mentally Unhealthy Days**; Recent mentally unhealthy days among adults aged >= 18 years.
|
| **[M]** <span style="color: #FF0000">OVWINC</span> (numeric): **Overweight Incidence**; Overweight or obesity among adults aged >= 18 years.
|
| **[N]** <span style="color: #FF0000">HLTINS</span> (numeric): **Health Insurance**; Current lack of health insurance among adults aged 18-64 years.
|
| **[O]** <span style="color: #FF0000">SMPREV</span> (numeric): **Smoking Prevalence**; Current smoking among adults aged >= 18 years.
|
##  1.2 Methodology
|
###  1.2.1 Data Assessment
|
| Preliminary data used in the study was evaluated and prepared for analysis and modelling using the following methods: 
|
| [Kaiser-Meyer-Olkin Factor Adequacy](https://link.springer.com/article/10.1007/BF02291817) evaluates whether the observed variables are suitable for exploratory factor analysis based on their common variance and the potential for extracting meaningful factors. The criterion is computed by examining the ratio of the sum of squared correlations between variables to the sum of squared partial correlations. A KMO value closer to 1 suggests that the variables have high shared variance and are suitable to proceed with the analysis.
|
| [Bartlettâ€™s Test of Sphericity](https://www.semanticscholar.org/paper/THE-EFFECT-OF-STANDARDIZATION-ON-A-%CF%872-APPROXIMATION-Bartlett/95d549d2c055360b34cc7d1fce739179c29e39bb) evaluates whether the correlations between variables in a data set are significant enough to support the assumption that underlying factors exist and can be extracted. The test calculates a Chi-Square statistic based on the differences between the observed correlation matrix and an identity matrix. The larger the Chi-Square value, there is more evidence against the null hypothesis stating that the correlation matrix is an identity matrix which indicates that the variables are uncorrelated and do not have any underlying structure.
|
| [Determinant Computation](https://www.taylorfrancis.com/books/mono/10.4324/9781003120001/step-step-guide-exploratory-factor-analysis-rstudio-marley-watkins) reflects the extent of multicollinearity among the variables in a correlation matrix. An extremely small determinant value indicates that the variables are highly correlated and nearly linearly dependent which can lead to unstable results and difficulty in interpreting their individual contributions.
|
| [Covariance Validity](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) evaluates whether the ratio of associated variables in the data set are sufficient enough to support the assumption that correlations exist. The criterion is computed by determining the proportion of correlation coefficients with values of at least 30% between all pairs of variables in the data set. A value closer to 1 suggests an adequate percentage of pairwise-correlated variables.
|
###  1.2.2 Model Formulation
|
| [Exploratory Factor Analysis](https://www.amazon.sg/Exploratory-Confirmatory-Factor-Analysis-Understanding/dp/1591470935) explores the underlying structure of a set of observed variables without specifying a pre-defined model. It allows for the identification of factors based on the patterns of correlations among observed variables. The method helps determine the number of factors that best explain the observed variance in the data, with the results used to decide how many factors to retain based on statistical criteria and contextual meaning. The analysis reveals the interrelationships among factors but does not test a specific model of factor structure. Factor loadings are derived - indicating how strongly each observed variable is associated with each factor. These loadings are interpreted to understand the structure of the data.
|
| [Confirmatory Factor Analysis](https://www.amazon.sg/Exploratory-Confirmatory-Factor-Analysis-Understanding/dp/1591470935) tests a pre-specified model of the relationships between observed variables and latent factors. It aims to confirm or reject the fit of a hypothesized factor structure based on theoretical considerations or previous research. The method requires the specification of a pre-determined factor structure, including the number of factors, the relationships between factors, and the loadings of observed variables on those factors. The focus of the analysis is on evaluating how well the chosen model fits the observed data.
|
| [Principal Axes Factor Extraction](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) identifies the underlying constructs that explain the observed correlations among variables by capturing both common variance (shared among variables) and unique variance (specific to each variable). This process potentially results to factors with lower communalities (explained variance) but with more direct interpretability. The algorithm performs eigenvalue decomposition on the correlation matrix. The eigenvalues represent the amount of variance explained by each eigenvector. Given a defined number of factors, loadings are calculated for each observed variable on each extracted factor. Factor loadings indicate the strength and direction of the relationship between variables and factors. Factors are interpreted based on the loading patterns. Variables with high loadings on a factor are strongly associated with the factor.
|
| [Maximum Likelihood Factor Extraction](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) aims to estimate the factor loadings in a way that maximizes the likelihood of observing the given data, assuming a specific factor model. Given the correlation matrix, the algorithm formulates a likelihood function that represents the probability of observing the given data under an assumed factor model representing the relationships between the latent factors and observed variables. The likelihood function quantifies how well the model explains the observed data. Optimization techniques are applied to determine the factor loadings that maximize the likelihood function. The process involves iteratively adjusting the factor loadings to improve the fit between the model and the data. Factor loadings indicate the strength and direction of the relationship between variables and factors. Factors are interpreted based on the loading patterns. Variables with high loadings on a factor are strongly associated with the factor.
|
| [Varimax Rotation](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) is an orthogonal rotation method which forces the rotated factors to be uncorrelated with each other, leading to simpler and more easily interpretable factor solutions. The algorithm aims to maximize the variance of the squared loadings within each factor which helps identify variables that are strongly associated with a single factor. The results are straightforward to interpret and can be particularly useful when the factors are expected to be independent.
|
| [Promax Rotation](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) is an oblique rotation method which allows for more flexibility by accommodating the possibility of correlated factors. The algorithm aims to simplify the factor structure by both maximizing the variance of the squared loadings within each factor and allowing for correlated factors. It uses a more complex mathematical approach to find the optimal rotation that accounts for both variance and correlation. The results provide a more accurate representation of the underlying relationships when the factors are expected to be correlated.
|
###  1.2.3 Model Performance Evaluation
|
| [Tucker Lewis Index](https://www.amazon.sg/Exploratory-Confirmatory-Factor-Analysis-Understanding/dp/1591470935) (TLI) is an incremental fit metric which assesses the goodness of fit of a proposed confirmatory factor analysis model to the observed data by comparing the fit of the specified model with the fit of a baseline model. The formula involves computing the ratio of the improvement in fit from the proposed model to the fit of the null model using non-normed measurements, while taking into account model complexity. Values may fall below zero or be above one with higher measurements indicating a better model fit.
|
| [Comparative Fit Index](https://www.amazon.sg/Exploratory-Confirmatory-Factor-Analysis-Understanding/dp/1591470935) (CFI) is an incremental fit metric which which assesses the goodness of fit of a proposed confirmatory factor analysis model to the observed data by comparing the fit of the specified model with the fit of a baseline model. The formula involves computing the ratio of the improvement in fit from the proposed model to the fit of the null model using normed measurements, with relativere insensitivity to model complexity. The metric values range between 0 and 1 with higher measurements indicating a better model fit.
|
| [Standardized Root Mean Square of the Residual](https://www.amazon.sg/Exploratory-Confirmatory-Factor-Analysis-Understanding/dp/1591470935) (SRMR) provides a measure of the average standardized residual, representing the average discrepancy between the observed and predicted covariance matrices. The metric is calculated by taking the square root of the average of the squared differences between the observed and predicted covariance matrices, standardized by the observed variable variances. Values range from 0 to 1, with lower values indicating better model fit. The SRMR is particularly sensitive to misspecifications in the model's covariances, making it a useful index for assessing the fit of the covariance structure.
|
| [Root Mean Square Error of Approximation](https://www.amazon.sg/Exploratory-Confirmatory-Factor-Analysis-Understanding/dp/1591470935) (RMSEA) provides a measure of the discrepancy between the observed and predicted covariances, adjusted for the complexity of the model by incorporating both fit and parsimony. Metric values range from 0 to positive infinity with lower values indicating better model fit. The RMSEA is considered a measure of "fit per degree of freedom," providing a balance between model fit and model complexity. It focuses on the error of approximation, quantifying how well the model approximates the observed covariance structure, and penalizes complex models that do not significantly improve fit over simpler models.
|
| [Bayesian Information Criterion](https://www.amazon.sg/Exploratory-Confirmatory-Factor-Analysis-Understanding/dp/1591470935) (BIC) is a statistical criterion used for model selection by balancing the goodness of fit of a model with its complexity, penalizing models with a larger number of parameters. In exploratory factor analysis where models with vary in terms of the numbers of factors or latent variables, BIC helps in comparing these models and selecting the one that achieves a good fit while avoiding overfitting. The metric is calculated using a combination of the maximized log-likelihood of the model, number of estimated parameters in the model, and the sample size. It incorporates a penalty term for model complexity, favoring models that achieve a good fit while using a parsimonious number of parameters.
|
###  1.2.4 Model Presentation
|
| [Dandelion Plots](https://cran.r-project.org/web/packages/DandEFA/DandEFA.pdf) provide a simultaneous visualization of both factor variances and loadings in an exploratory factor analysis. Each central line represents a different factor and is connected to a star graph. These star graphs visualize the factor loadings for the corresponding factor with negative and positive loadings indicated by different colors. Explained variance of each factor can be observed by the size of each star graph or by the angle between the current and the consecutive central line.  A comparison of the derived communalities and uniquenesses is also provided together with the cumulative explanation ratios of factors.
|
| [Path Diagrams](https://cran.r-project.org/web/packages/lavaan/lavaan.pdf) provide a visual representation of the hypothesized relationships among latent (unobserved) factors and their observed indicators in a confirmatory factor analysis by illustrating the structure of the model and indicating how the latent factors influence the observed variables. The key components of a path diagram include unobserved constructs that are postulated to underlie the observed variability in a set of measured variables represented by circles or ovals, observed indicators assumed to be influenced by the underlying latent factors represented by rectangles or squares, arrows connecting latent factors to observed indicators representing factor loadings suggesting that the latent factor causes variation in the observed variable, numerical values assigned to the arrows indicating the strength and direction of the relationship between the latent factor and the observed variable, and double-headed arrows connecting two latent factors representing the covariance between those factors.
|
##  1.3 Results
|
###  1.3.1 Data Preparation
|
| **[A]** The initial tabular dataset was comprised of 50 observations and 16 variables (including 1 metadata and 30 descriptors). 
|      **[A.1]** 50 rows (observations)
|      **[A.2]** 16 columns (variables)
|             **[A.2.1]** 1/16 instance labels = <span style="color: #FF0000">STATE</span> (character)
|             **[A.2.2]** 15/16 predictors = 15/15 numeric
|                      **[A.2.2.1]** <span style="color: #FF0000">LDMORT</span> (numeric)
|                      **[A.2.2.2]** <span style="color: #FF0000">ARTINC</span> (numeric)
|                      **[A.2.2.3]** <span style="color: #FF0000">ASMORT</span> (numeric)
|                      **[A.2.2.4]** <span style="color: #FF0000">PSMUSE</span> (numeric)
|                      **[A.2.2.5]** <span style="color: #FF0000">RDMORT</span> (numeric)
|                      **[A.2.2.6]** <span style="color: #FF0000">PDMORT</span> (numeric)
|                      **[A.2.2.7]** <span style="color: #FF0000">CDMORT</span> (numeric)
|                      **[A.2.2.8]** <span style="color: #FF0000">HDMORT</span> (numeric)
|                      **[A.2.2.9]** <span style="color: #FF0000">STMORT</span> (numeric)
|                      **[A.2.2.10]** <span style="color: #FF0000">DBMORT</span> (numeric)
|                      **[A.2.2.11]** <span style="color: #FF0000">INFVAC</span> (numeric)
|                      **[A.2.2.12]** <span style="color: #FF0000">MEUNDA</span> (numeric)
|                      **[A.2.2.13]** <span style="color: #FF0000">OVWINC</span> (numeric)
|                      **[A.2.2.14]** <span style="color: #FF0000">HLTINS</span> (numeric)
|                      **[A.2.2.15]** <span style="color: #FF0000">SMPREV</span> (numeric)
|
| 

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.1, warning=FALSE, message=FALSE}
##################################
# Loading R libraries
##################################
library(AppliedPredictiveModeling)
library(performance)
library(parameters)
library(HH)
library(tidyr)
library(caret)
library(psych)
library(lattice)
library(dplyr)
library(moments)
library(skimr)
library(RANN)
library(pls)
library(corrplot)
library(lares)
library(DMwR)
library(gridExtra)
library(rattle)
library(RColorBrewer)
library(stats)
library(factoextra)
library(FactoMineR)
library(gplots)
library(qgraph)
library(ggplot2)
library(GGally)
library(psych)
library(nFactors)
library(MBESS)
library(mice)
library(DandEFA)
library(EFAtools)
library(tidyverse)
library(lavaan)
library(semPlot)
library(semoutput)
library(sjPlot)
library(GPArotation)
library(semTools)

##################################
# Loading source and
# formulating the analysis set
##################################
CDI <- read.csv("ChronicDiseaseIndicators.csv",
                      na.strings=c("NA","NaN"," ",""),
                      stringsAsFactors = FALSE)
CDI <- as.data.frame(CDI)

##################################
# Performing a general exploration of the data set
##################################
dim(CDI)
str(CDI)
summary(CDI)

##################################
# Formulating a data type assessment summary
##################################
PDA <- CDI
(PDA.Summary <- data.frame(
  Column.Index=c(1:length(names(PDA))),
  Column.Name= names(PDA), 
  Column.Type=sapply(PDA, function(x) class(x)), 
  row.names=NULL)
)

```

</details>

###  1.3.2 Data Quality Assessment
|
| **[A]** Missing data noted for 1 variable with Null.Count>0 and Fill.Rate<1.0.
|      **[A.1]** <span style="color: #FF0000">ASMORT</span>: Null.Count=11, Fill.Rate=0.78
| **[B]** No low variance observed for any variable with First.Second.Mode.Ratio>5.
| **[C]** No low variance observed for any variable with Unique.Count.Ratio<0.01.
| **[D]** No high skewness observed for any variable with Skewness>3 or Skewness<(-3).
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DQA <- CDI

##################################
# Formulating an overall data quality assessment summary
##################################
(DQA.Summary <- data.frame(
  Column.Name= names(DQA),
  Column.Type=sapply(DQA, function(x) class(x)),
  Row.Count=sapply(DQA, function(x) nrow(DQA)),
  NA.Count=sapply(DQA,function(x)sum(is.na(x))),
  Fill.Rate=sapply(DQA,function(x)format(round((sum(!is.na(x))/nrow(DQA)),3),nsmall=3)),
  row.names=NULL)
)

##################################
# Listing all descriptors
##################################
DQA.Descriptors <- DQA[,colnames(DQA)!="State"]

##################################
# Listing all numeric Descriptors
##################################
DQA.Descriptors.Numeric <- DQA.Descriptors[,sapply(DQA.Descriptors, is.numeric)]

if (length(names(DQA.Descriptors.Numeric))>0) {
    print(paste0("There are ",
               (length(names(DQA.Descriptors.Numeric))),
               " numeric descriptor variable(s)."))
} else {
  print("There are no numeric descriptor variables.")
}

##################################
# Listing all factor Descriptors
##################################
DQA.Descriptors.Factor <- DQA.Descriptors[,sapply(DQA.Descriptors, is.factor)]

if (length(names(DQA.Descriptors.Factor))>0) {
    print(paste0("There are ",
               (length(names(DQA.Descriptors.Factor))),
               " factor descriptor variable(s)."))
} else {
  print("There are no factor descriptor variables.")
}

##################################
# Formulating a data quality assessment summary for factor Descriptors
##################################
if (length(names(DQA.Descriptors.Factor))>0) {

  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = x[!(x %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return("x"),
           return(usm[tabsm == max(tabsm)]))
  }

  (DQA.Descriptors.Factor.Summary <- data.frame(
  Column.Name= names(DQA.Descriptors.Factor),
  Column.Type=sapply(DQA.Descriptors.Factor, function(x) class(x)),
  Unique.Count=sapply(DQA.Descriptors.Factor, function(x) length(unique(x))),
  First.Mode.Value=sapply(DQA.Descriptors.Factor, function(x) as.character(FirstModes(x)[1])),
  Second.Mode.Value=sapply(DQA.Descriptors.Factor, function(x) as.character(SecondModes(x)[1])),
  First.Mode.Count=sapply(DQA.Descriptors.Factor, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Descriptors.Factor, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  Unique.Count.Ratio=sapply(DQA.Descriptors.Factor, function(x) format(round((length(unique(x))/nrow(DQA.Descriptors.Factor)),3), nsmall=3)),
  First.Second.Mode.Ratio=sapply(DQA.Descriptors.Factor, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  row.names=NULL)
  )

}

##################################
# Formulating a data quality assessment summary for numeric Descriptors
##################################
if (length(names(DQA.Descriptors.Numeric))>0) {

  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = na.omit(x)[!(na.omit(x) %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return(0.00001),
           return(usm[tabsm == max(tabsm)]))
  }

  (DQA.Descriptors.Numeric.Summary <- data.frame(
  Column.Name= names(DQA.Descriptors.Numeric),
  Column.Type=sapply(DQA.Descriptors.Numeric, function(x) class(x)),
  Unique.Count=sapply(DQA.Descriptors.Numeric, function(x) length(unique(x))),
  Unique.Count.Ratio=sapply(DQA.Descriptors.Numeric, function(x) format(round((length(unique(x))/nrow(DQA.Descriptors.Numeric)),3), nsmall=3)),
  First.Mode.Value=sapply(DQA.Descriptors.Numeric, function(x) format(round((FirstModes(x)[1]),3),nsmall=3)),
  Second.Mode.Value=sapply(DQA.Descriptors.Numeric, function(x) format(round((SecondModes(x)[1]),3),nsmall=3)),
  First.Mode.Count=sapply(DQA.Descriptors.Numeric, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Descriptors.Numeric, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  First.Second.Mode.Ratio=sapply(DQA.Descriptors.Numeric, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  Minimum=sapply(DQA.Descriptors.Numeric, function(x) format(round(min(x,na.rm = TRUE),3), nsmall=3)),
  Mean=sapply(DQA.Descriptors.Numeric, function(x) format(round(mean(x,na.rm = TRUE),3), nsmall=3)),
  Median=sapply(DQA.Descriptors.Numeric, function(x) format(round(median(x,na.rm = TRUE),3), nsmall=3)),
  Maximum=sapply(DQA.Descriptors.Numeric, function(x) format(round(max(x,na.rm = TRUE),3), nsmall=3)),
  Skewness=sapply(DQA.Descriptors.Numeric, function(x) format(round(skewness(x,na.rm = TRUE),3), nsmall=3)),
  Kurtosis=sapply(DQA.Descriptors.Numeric, function(x) format(round(moments::kurtosis(x,na.rm = TRUE),3), nsmall=3)),
  Percentile25th=sapply(DQA.Descriptors.Numeric, function(x) format(round(quantile(x,probs=0.25,na.rm = TRUE),3), nsmall=3)),
  Percentile75th=sapply(DQA.Descriptors.Numeric, function(x) format(round(quantile(x,probs=0.75,na.rm = TRUE),3), nsmall=3)),
  row.names=NULL)
  )

}

##################################
# Identifying potential data quality issues
##################################

##################################
# Checking for missing observations
##################################
if ((nrow(DQA.Summary[DQA.Summary$NA.Count>0,]))>0){
  print(paste0("Missing observations noted for ",
               (nrow(DQA.Summary[DQA.Summary$NA.Count>0,])),
               " variable(s) with NA.Count>0 and Fill.Rate<1.0."))
  DQA.Summary[DQA.Summary$NA.Count>0,]
} else {
  print("No missing observations noted.")
}

##################################
# Checking for zero or near-zero variance Descriptors
##################################
if (length(names(DQA.Descriptors.Factor))==0) {
  print("No factor descriptors noted.")
} else if (nrow(DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,])),
               " factor variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance factor descriptors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,])),
               " numeric variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance numeric descriptors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,])),
               " numeric variable(s) with Unique.Count.Ratio<0.01."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,]
} else {
  print("No low variance numeric descriptors due to low unique count ratio noted.")
}

##################################
# Checking for skewed Descriptors
##################################
if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),])>0){
  print(paste0("High skewness observed for ",
  (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),])),
  " numeric variable(s) with Skewness>3 or Skewness<(-3)."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                 as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),]
} else {
  print("No skewed numeric descriptors noted.")
}

```

</details>

###  1.3.3 Data Preprocessing
|
|
####  1.3.3.1 Missing Data Imputation
|
| **[A]** Missing data identified for 1 out of the 30 descriptors were replaced using multivariate imputation by chained equations with predictive mean matching specified as the imputation method.
|      **[A.1]** <span style="color: #FF0000">ASMORT</span>: Null.Count=0, Fill.Rate=1.00
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.1, warning=FALSE, message=FALSE}
##################################
# Visualizing the missing data patterns
# prior to imputation
##################################
summary(DQA.Descriptors.Numeric)
visdat::vis_miss(DQA.Descriptors.Numeric, sort_miss = FALSE)

##################################
# Conducting missing data imputation
# using Multivariate Imputation by Chained Equations
##################################
MICE_Model <- mice(DQA.Descriptors.Numeric, method='pmm', seed = 123)
DQA.Descriptors.Numeric <- complete(MICE_Model)

##################################
# Visualizing the missing data patterns
# after imputation
##################################
visdat::vis_miss(DQA.Descriptors.Numeric, sort_miss = FALSE)

```
</details>

|
|
####  1.3.3.2 Outlier Detection
|
| **[A]** Minimal outliers noted for 7 out of the 15 descriptors. Descriptor values were visualized through a boxplot including observations classified as suspected outliers using the IQR criterion. The IQR criterion means that all observations above the (75th percentile + 1.5 x IQR) or below the (25th percentile - 1.5 x IQR) are suspected outliers, where IQR is the difference between the third quartile (75th percentile) and first quartile (25th percentile).
|      **[A.1]** <span style="color: #FF0000">LDMORT</span> = 1
|      **[A.2]** <span style="color: #FF0000">ARTINC</span> = 4
|      **[A.3]** <span style="color: #FF0000">RDMORT</span> = 2
|      **[A.5]** <span style="color: #FF0000">PDMORT</span> = 1
|      **[A.7]** <span style="color: #FF0000">DBMORT</span> = 1
|      **[A.9]** <span style="color: #FF0000">HLTINS</span> = 1
|      **[A.10]** <span style="color: #FF0000">SMPREV</span> = 1
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.2.1, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- DQA.Descriptors.Numeric

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA))
```

```{r section_1.3.3.2.2, warning=FALSE, message=FALSE, fig.width=15, fig.height=2}
##################################
# Outlier Detection
##################################

##################################
# Listing all Descriptors
##################################
DPA.Descriptors <- DPA

##################################
# Listing all numeric Descriptors
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors[,sapply(DPA.Descriptors, is.numeric)]

##################################
# Identifying outliers for the numeric Descriptors
##################################
OutlierCountList <- c()

for (i in 1:ncol(DPA.Descriptors.Numeric)) {
  Outliers <- boxplot.stats(DPA.Descriptors.Numeric[,i])$out
  OutlierCount <- length(Outliers)
  OutlierCountList <- append(OutlierCountList,OutlierCount)
  OutlierIndices <- which(DPA.Descriptors.Numeric[,i] %in% c(Outliers))
  print(
  ggplot(DPA.Descriptors.Numeric, aes(x=DPA.Descriptors.Numeric[,i])) +
  geom_boxplot() +
  theme_bw() +
  theme(axis.text.y=element_blank(), 
        axis.ticks.y=element_blank()) +
  xlab(names(DPA.Descriptors.Numeric)[i]) +
  labs(title=names(DPA.Descriptors.Numeric)[i],
       subtitle=paste0(OutlierCount, " Outlier(s) Detected")))
}

```

</details>

|
|
####  1.3.3.3 Zero and Near-Zero Variance
|
| **[A]** No low variance observed for any descriptor in the dataset.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.3, warning=FALSE, message=FALSE}
##################################
# Zero and Near-Zero Variance
##################################

##################################
# Identifying columns with low variance
###################################
DPA_LowVariance <- nearZeroVar(DPA,
                               freqCut = 80/20,
                               uniqueCut = 10,
                               saveMetrics= TRUE)
(DPA_LowVariance[DPA_LowVariance$nzv,])

if ((nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))==0){
  
  print("No low variance descriptors noted.")
  
} else {

  print(paste0("Low variance observed for ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s) with First.Second.Mode.Ratio>4 and Unique.Count.Ratio<0.10."))
  
  DPA_LowVarianceForRemoval <- (nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))
  
  print(paste0("Low variance can be resolved by removing ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LowVarianceForRemoval) {
  DPA_LowVarianceRemovedVariable <- rownames(DPA_LowVariance[DPA_LowVariance$nzv,])[j]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LowVarianceRemovedVariable))
  }
  
  DPA %>%
  skim() %>%
  dplyr::filter(skim_variable %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,]))

}
```

</details>

|
|
####  1.3.3.4 Collinearity
|
| **[A]** High multicollinearity with Pearson correlation coefficients >90% was noted in a pair of descriptors in the dataset.
|      **[A.1]** <span style="color: #FF0000">HDMORT</span> and <span style="color: #FF0000">CDMORT</span> = 0.99
|      **[A.2]** <span style="color: #FF0000">CDMORT</span> was removed based on its higher correlation with other descriptors.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.4, warning=FALSE, message=FALSE}
##################################
# Measuring pairwise correlation between descriptors
##################################
(DPA_Correlation <- cor(DPA.Descriptors.Numeric,
                        method = "pearson",
                        use="pairwise.complete.obs"))

##################################
# Testing pairwise correlation between descriptors
##################################
DPA_CorrelationTest <- cor.mtest(DPA.Descriptors.Numeric,
                       method = "pearson",
                       conf.level = 0.95)

##################################
# Visualizing pairwise correlation between descriptors
##################################
corrplot(cor(DPA.Descriptors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"),
             method = "circle",
             type = "upper",
             order = "original",
             tl.col = "black",
             tl.cex = 0.75,
             tl.srt = 90,
             sig.level = 0.05,
             p.mat = DPA_CorrelationTest$p,
             insig = "blank")

corrplot(cor(DPA.Descriptors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"),
             method = "number",
             type = "upper",
             order = "original",
             tl.col = "black",
             tl.cex = 0.75,
             tl.srt = 90,
             sig.level = 0.05,
             p.mat = DPA_CorrelationTest$p,
             insig = "blank")

##################################
# Identifying the highly correlated variables
##################################
(DPA_HighlyCorrelatedCount <- sum(abs(DPA_Correlation[upper.tri(DPA_Correlation)])>0.90))

if (DPA_HighlyCorrelatedCount > 0) {
  DPA_HighlyCorrelated <- findCorrelation(DPA_Correlation, cutoff = 0.90)
  
  (DPA_HighlyCorrelatedForRemoval <- length(DPA_HighlyCorrelated))
  
  print(paste0("High correlation can be resolved by removing ",
               (DPA_HighlyCorrelatedForRemoval),
               " numeric variable(s)."))
  
  for (j in 1:DPA_HighlyCorrelatedForRemoval) {
    DPA_HighlyCorrelatedRemovedVariable <- colnames(DPA.Descriptors.Numeric)[DPA_HighlyCorrelated[j]]
    print(paste0("Variable ",
                 j,
                 " for removal: ",
                 DPA_HighlyCorrelatedRemovedVariable))
  }
  
}

if (DPA_HighlyCorrelatedCount == 0) {
  print("No highly correlated predictors noted.")
} else {
  print(paste0("High correlation observed for ",
               (DPA_HighlyCorrelatedCount),
               " pairs of numeric variable(s) with Correlation.Coefficient>0.90."))
  
  (DPA_HighlyCorrelatedPairs <- corr_cross(DPA.Descriptors.Numeric,
  max_pvalue = 0.05, 
  top = DPA_HighlyCorrelatedCount,
  rm.na = TRUE,
  grid = FALSE
))
  
}

##################################
# Removing individual descriptors
# among the highly correlated pair
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("CDMORT")]
```

</details>

|
|
####  1.3.3.5 Linear Dependencies
|
| **[A]** No linear dependencies observed for any subset of decriptors in the dataset. 
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.5, warning=FALSE, message=FALSE}
##################################
# Linear Dependencies
##################################

##################################
# Finding linear dependencies
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Descriptors.Numeric)

##################################
# Identifying the linearly dependent variables
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Descriptors.Numeric)

(DPA_LinearlyDependentCount <- length(DPA_LinearlyDependent$linearCombos))

if (DPA_LinearlyDependentCount == 0) {
  print("No linearly dependent predictors noted.")
} else {
  print(paste0("Linear dependency observed for ",
               (DPA_LinearlyDependentCount),
               " subset(s) of numeric variable(s)."))
  
  for (i in 1:DPA_LinearlyDependentCount) {
    DPA_LinearlyDependentSubset <- colnames(DPA.Descriptors.Numeric)[DPA_LinearlyDependent$linearCombos[[i]]]
    print(paste0("Linear dependent variable(s) for subset ",
                 i,
                 " include: ",
                 DPA_LinearlyDependentSubset))
  }
  
}

##################################
# Identifying the linearly dependent variables for removal
##################################

if (DPA_LinearlyDependentCount > 0) {
  DPA_LinearlyDependent <- findLinearCombos(DPA.Descriptors.Numeric)
  
  DPA_LinearlyDependentForRemoval <- length(DPA_LinearlyDependent$remove)
  
  print(paste0("Linear dependency can be resolved by removing ",
               (DPA_LinearlyDependentForRemoval),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LinearlyDependentForRemoval) {
  DPA_LinearlyDependentRemovedVariable <- colnames(DPA.Descriptors.Numeric)[DPA_LinearlyDependent$remove[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LinearlyDependentRemovedVariable))
  }

}

```

</details>

|
|
####  1.3.3.6 Distributional Shape
|
| **[A]** No shape transformation was necessary as the distributional skewness observed among individual descriptors was normal (all values within +3 and -3) with minimal outliers.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.6, warning=FALSE, message=FALSE, fig.width=15, fig.height=2}
##################################
# Distributional Shape
##################################

##################################
# Formulating the histogram
# for the numeric descriptors
##################################
for (i in 1:ncol(DPA.Descriptors.Numeric)) {
  Median <- format(round(median(DPA.Descriptors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  Mean <- format(round(mean(DPA.Descriptors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  Skewness <- format(round(skewness(DPA.Descriptors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  print(
  ggplot(DPA.Descriptors.Numeric, aes(x=DPA.Descriptors.Numeric[,i])) +
  geom_histogram(binwidth=1,color="black", fill="white") +
  geom_vline(aes(xintercept=mean(DPA.Descriptors.Numeric[,i])),
            color="blue", size=1) +
    geom_vline(aes(xintercept=median(DPA.Descriptors.Numeric[,i])),
            color="red", size=1) +
  theme_bw() +
  ylab("Count") +
  xlab(names(DPA.Descriptors.Numeric)[i]) +
  labs(title=names(DPA.Descriptors.Numeric)[i],
       subtitle=paste0("Median = ", Median,
                       ", Mean = ", Mean,
                       ", Skewness = ", Skewness)))
}

```

</details>

|
|
####  1.3.3.7 Data Standardization
|
| **[A]** Z-score normalization was applied to generate a transformed data with a distributional mean of zero and a standard deviation equal to one - allowing a consistent scale among the descriptors.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.7, warning=FALSE, message=FALSE, fig.width=15, fig.height=2}
##################################
# Data Normalization
##################################
DPA.Descriptors.Numeric.CenteredScaled <- preProcess(DPA.Descriptors.Numeric, method = c("center","scale"))
DPA.Descriptors.Numeric <- predict(DPA.Descriptors.Numeric.CenteredScaled, DPA.Descriptors.Numeric)

```

</details>

###  1.3.4 Data Pre-Assessment
|
|
####  1.3.4.1 Correlation Matrix Assessment - Kaiser-Meyer-Olkin Factor Adequacy
|
| **[A]** 5 descriptors with low KMO values were excluded from the dataset to improve the strength and patterns of relationships between variables and ensure sufficient common variance among variables for extracting underlying factors.
|      **[A.1]** <span style="color: #FF0000">ASMORT</span> = 0.15
|      **[A.2]** <span style="color: #FF0000">INFVAC</span> = 0.35
|      **[A.3]** <span style="color: #FF0000">HLTINS</span> = 0.56
|      **[A.4]** <span style="color: #FF0000">LDMORT</span> = 0.47
|      **[A.5]** <span style="color: #FF0000">PSMUSE</span> = 0.59
|
| **[B]** The KMO measure of sampling adequacy was acceptable with a computed value of 0.87 for the complete model, indicating the suitability of the data for exploratory factor analysis. The estimated proportion of variance among all the observed variables was sufficiently adequate.
|
| **[C]**  The sampling adequacy for each variable in the model was acceptable with KMO values ranging from 0.78 to 0.94. Results indicated that each variable can be sufficiently predicted by other variables. 
|      **[C.1]** <span style="color: #FF0000">PDMORT</span> = 0.94
|      **[C.2]** <span style="color: #FF0000">DBMORT</span> = 0.92
|      **[C.3]** <span style="color: #FF0000">STMORT</span> = 0.90
|      **[C.4]** <span style="color: #FF0000">HDMORT</span> = 0.89
|      **[C.5]** <span style="color: #FF0000">RDMORT</span> = 0.88
|      **[C.6]** <span style="color: #FF0000">SMPREV</span> = 0.88
|      **[C.7]** <span style="color: #FF0000">OVWINC</span> = 0.85
|      **[C.8]** <span style="color: #FF0000">ARTINC</span> = 0.83
|      **[C.9]** <span style="color: #FF0000">MEUNDA</span> = 0.78
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.4.1, warning=FALSE, message=FALSE}
##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Removing individual descriptors
# with low KMO values
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("ASMORT")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Removing individual descriptors
# with low KMO values
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("INFVAC")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Removing individual descriptors
# with low KMO values
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("HLTINS")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Removing individual descriptors
# with low KMO values
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("LDMORT")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Removing individual descriptors
# with low KMO values
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("PSMUSE")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

```

</details>

|
|
####  1.3.4.2 Correlation Matrix Assessment - Bartlettâ€™s Test of Sphericity
|
| **[A]** The computed p-value from the Bartlettâ€™s Test of Sphericity was statistically significant (<0.00001), rejecting the null hypothesis that the correlation matrix is an identity matrix (ones on the diagonal and zeros on the off-diagonal). Results indicated that there is enough evidence to support the existence of underlying factors, suggesting that the correlation matrix is appropriate for exploratory factor analysis.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.4.2, warning=FALSE, message=FALSE}
##################################
# Calculating the Bartlett's Test of Sphericity
##################################
(DPA_BartlettTest <- cortest.bartlett(DPA.Descriptors.Numeric,
                                      n=nrow(DPA.Descriptors.Numeric)))

```

</details>

|
|
####  1.3.4.3 Correlation Matrix Assessment - Covariance Validity
|
| **[A]** Covariance among descriptors in the correlation matrix was sufficient to justify the conduct of an exploratory factor analysis. 97% (35/36) of the pairwise associations using the Pearson correlation coefficient were above 30%.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.4.3, warning=FALSE, message=FALSE}
##################################
# Measuring pairwise correlation between descriptors
##################################
(DPA_Correlation <- cor(DPA.Descriptors.Numeric,
                        method = "pearson",
                        use="pairwise.complete.obs"))

##################################
# Identifying the minimally correlated variables
##################################
(DPA_MinimallyCorrelatedCount <- sum(abs(DPA_Correlation[upper.tri(DPA_Correlation)])>0.30))

(DPA_AllPairs <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)

(DPA_MinimallyCorrelatedCountPercentage <- DPA_MinimallyCorrelatedCount/DPA_AllPairs)

qgraph(cor(DPA.Descriptors.Numeric),
       cut=0.30,
       details=FALSE,
       posCol="#2F75B5",
       negCol="#FF5050",
       labels=names(DPA.Descriptors.Numeric))

```

</details>
|
|
####  1.3.4.4 Correlation Matrix Assessment - Determinant Computation
|
| **[A]** The determinant of the correlation matrix was computed as 0.00080 which was greater than 0.00001 indicating the absence of the likelihood for a multicollinearity problem. The results allowed for matrix operations to produce stable results during exploratory factor analysis.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.4.4, warning=FALSE, message=FALSE}
##################################
# Computing the determinant of the correlation matrix
##################################
(DPA_CorrelationMatrixDeterminant <- det(cor(DPA.Descriptors.Numeric)))

```

</details>

|
|
####  1.3.4.5 Pre-Processed Dataset
|
| **[A]** The pre-processed tabular dataset was comprised of 50 observations and 9 descriptor variables. 
|      **[A.1]** 50 rows (observations)
|      **[A.2]** 9 columns (descriptors)
|             **[A.2.1]** <span style="color: #FF0000">PDMORT</span>
|             **[A.2.2]** <span style="color: #FF0000">DBMORT</span>
|             **[A.2.3]** <span style="color: #FF0000">STMORT</span>
|             **[A.2.4]** <span style="color: #FF0000">HDMORT</span>
|             **[A.2.5]** <span style="color: #FF0000">RDMORT</span>
|             **[A.2.6]** <span style="color: #FF0000">SMPREV</span>
|             **[A.2.7]** <span style="color: #FF0000">OVWINC</span>
|             **[A.2.8]** <span style="color: #FF0000">ARTINC</span>
|             **[A.2.9]** <span style="color: #FF0000">MEUNDA</span>
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.4.5, warning=FALSE, message=FALSE}
##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA.Descriptors.Numeric))

```

</details>

###  1.3.5 Data Exploration
|
|
####  1.3.5.1 Correlation Matrix
|
| **[A]** All pairwise correlations among descriptors were positive with values ranging from 0.28 to 0.82 and were determined as statistically significant. 
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.5.1.1, warning=FALSE, message=FALSE, fig.width=15, fig.height=2}
##################################
# Formulating the histogram
# for the numeric descriptors
##################################
for (i in 1:ncol(DPA.Descriptors.Numeric)) {
  Median <- format(round(median(DPA.Descriptors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  Mean <- format(round(mean(DPA.Descriptors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  Skewness <- format(round(skewness(DPA.Descriptors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  print(
  ggplot(DPA.Descriptors.Numeric, aes(x=DPA.Descriptors.Numeric[,i])) +
  geom_histogram(binwidth=0.25, color="black", fill="white", breaks = seq(-4, 4, by = 1)) +
    scale_x_continuous(breaks = seq(-4, 4, 1)) +
  geom_vline(aes(xintercept=mean(DPA.Descriptors.Numeric[,i])),
            color="blue", size=1) +
    geom_vline(aes(xintercept=median(DPA.Descriptors.Numeric[,i])),
            color="red", size=1) +
  theme_bw() +
  ylab("Count") +
  xlab(names(DPA.Descriptors.Numeric)[i]) +
  labs(title=names(DPA.Descriptors.Numeric)[i],
       subtitle=paste0("Median = ", Median,
                       ", Mean = ", Mean,
                       ", Skewness = ", Skewness)))
}

```

```{r section_1.3.5.1.2, warning=FALSE, message=FALSE}
##################################
# Plotting the scatterplot matrix
# between descriptors
##################################
DPA_ScatterplotMatrix <- ggpairs(DPA.Descriptors.Numeric, 
                                 upper=list(continuous="points"), 
                                 lower=list(continuous="smooth_loess"), 
                                 diag=list(continuous="barDiag"))

##################################
# Measuring pairwise correlation 
# between the updated descriptors
##################################
(DPA_Correlation <- cor(DPA.Descriptors.Numeric,
                        method = "pearson",
                        use="pairwise.complete.obs"))

##################################
# Setting the correlation breaks
##################################
DPA_Breaks = seq(0.25, 1.00, 0.01)

##################################
# Setting the correlation binnings
##################################
DPA_CorrelationBins <- .bincode(DPA_Correlation, 
                                DPA_Breaks, 
                                include.lowest = T)

##################################
# Transforming the correlation values
# into color ranges
##################################
DPA_ScatterplotMatrixColorRange <- matrix(factor(DPA_CorrelationBins, 
                                                 levels = 1:length(DPA_Breaks), 
                                                 labels = rev(colorpanel(length(DPA_Breaks),"#DA2A2A","#F2B2B2","#FBE5E5"))),
                                          DPA_ScatterplotMatrix$nrow)

##################################
# Updating the scatterplot matrix
# through color-coding depending
# on the correlation values
##################################
for(i in 1:DPA_ScatterplotMatrix$nrow) {
  for(j in 1:DPA_ScatterplotMatrix$ncol){
    DPA_ScatterplotMatrix[i,j] <- DPA_ScatterplotMatrix[i,j] + 
      theme(panel.background= element_rect(fill=DPA_ScatterplotMatrixColorRange[i,j]))
    
  if(i == j){
    DPA_ScatterplotMatrix[i,j] <- DPA_ScatterplotMatrix[i,j] + 
      theme(panel.background= element_rect(fill="#EBECF0"))
    }

}}

##################################
# Plotting the scatterplot matrix
# color-coded by correlation values
##################################
DPA_ScatterplotMatrix 

##################################
# Testing pairwise correlation between descriptors
##################################
DPA_CorrelationTest <- cor.mtest(DPA.Descriptors.Numeric,
                       method = "pearson",
                       conf.level = 0.95)

##################################
# Plotting the pairwise association
# between descriptors
##################################
corrplot(cor(DPA.Descriptors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"),
             method = "number",
             type = "upper",
             order = "original",
             tl.col = "black",
             tl.cex = 0.75,
             tl.srt = 90,
             sig.level = 0.05,
             p.mat = DPA_CorrelationTest$p,
             insig = "blank")

```

</details>

###  1.3.6 Model Development
|
|
####  1.3.6.1 Principal Axes Factor Extraction and Varimax Rotation
|
| **[A]** Appplying **Principal Axes** factor extraction and **Varimax** rotation, an evaluation was conducted using a set of empirical guidelines to determine the optimal number of factors to be retained for exploratory factor analysis. It was determined that:
|      **[A.1]** The choice of 3 factors was supported by the **Bentler**, **CNG**, **t** and **p** methods for determining the optimal number of factors.
|      **[A.2]** The choice of 4 factors was supported by the **Beta** and **Bartlett** methods for determining the optimal number of factors.
|      **[A.3]** 3 or 4 factors would be sufficiently plausible for an optimal balance between comprehensiveness and parsimony. 
|      **[A.4]** To ensure that both under-extraction and over-extraction are assessed, models with 3 and 4 and factors were sequentially evaluated for their interpretability and theoretical meaningfulness.

|
| **[B]** Results for the exploratory factor analysis using a **3-Factor Structure** were as follows:
|      **[B.1]** Standardized Root Mean Square of the Residual = 0.03
|      **[B.2]** Tucker-Lewis Fit Index = 0.98
|      **[B.3]** Bayesian Information Criterion = -33.48
|      **[B.4]** High Residual Rate = 0.11
|      **[B.5]** Factor 1 was a latent factor with higher loading towards the following descriptors:
|             **[B.5.1]** <span style="color: #FF0000">STMORT</span>: Loading = 0.82, Communality = 0.80
|             **[B.5.2]** <span style="color: #FF0000">OVWINC</span>: Loading = 0.73, Communality = 0.72
|             **[B.5.3]** <span style="color: #FF0000">RDMORT</span>: Loading = 0.70, Communality = 0.63
|             **[B.5.4]** <span style="color: #FF0000">HDMORT</span>: Loading = 0.57, Communality = 0.62
|             **[B.5.5]** <span style="color: #FF0000">DBMORT</span>: Loading = 0.52, Communality = 0.41
|             **[B.5.6]** Cronbach's Alpha = 0.88
|      **[B.6]** Factor 2 was a latent factor with higher loading towards the following descriptors:
|             **[B.6.1]** <span style="color: #FF0000">MEUNDA</span>: Loading = 0.99, Communality = 1.06
|             **[B.6.2]** <span style="color: #FF0000">ARTINC</span>: Loading = 0.62, Communality = 0.84
|             **[B.6.3]** Cronbach's Alpha = 0.89
|      **[B.7]** Factor 3 was a latent factor with higher loading towards the following descriptors:
|             **[B.7.1]** <span style="color: #FF0000">SMPREV</span>: Loading = 0.77, Communality = 0.90
|             **[B.7.2]** <span style="color: #FF0000">PDMORT</span>: Loading = 0.60, Communality = 0.62
|             **[B.7.3]** Cronbach's Alpha = 0.85
|
| **[C]** Results for the exploratory factor analysis using a **4-Factor Structure** were as follows:
|      **[C.1]** Standardized Root Mean Square of the Residual = 0.01
|      **[C.2]** Tucker-Lewis Fit Index = 1.08
|      **[C.3]** Bayesian Information Criterion = -21.18
|      **[C.4]** High Residual Rate = 0.05
|      **[C.5]** Factor 1 was a latent factor with higher loading towards the following descriptors:
|             **[C.5.1]** <span style="color: #FF0000">MEUNDA</span>: Loading = 0.90, Communality = 0.90
|             **[C.5.2]** <span style="color: #FF0000">ARTINC</span>: Loading = 0.70, Communality = 0.89
|             **[C.5.3]** Cronbach's Alpha = 0.89
|      **[C.6]** Factor 2 was a latent factor with higher loading towards the following descriptors:
|             **[C.6.1]** <span style="color: #FF0000">RDMORT</span>: Loading = 0.89, Communality = 0.95
|             **[C.6.2]** <span style="color: #FF0000">DBMORT</span>: Loading = 0.50, Communality = 0.44
|             **[C.6.3]** Standardized Cronbach's Alpha = 0.76
|      **[C.7]** Factor 3 was a latent factor with higher loading towards the following descriptors:
|             **[C.7.1]** <span style="color: #FF0000">SMPREV</span>: Loading = 0.73, Communality = 0.87
|             **[C.7.2]** <span style="color: #FF0000">PDMORT</span>: Loading = 0.61, Communality = 0.65
|             **[C.7.3]** Cronbach's Alpha = 0.85
|      **[C.8]** Factor 4 was a latent factor with higher loading towards the following descriptors:
|             **[C.8.1]** <span style="color: #FF0000">HDMORT</span>: Loading = 0.68, Communality = 0.79
|             **[C.8.2]** <span style="color: #FF0000">STMORT</span>: Loading = 0.60, Communality = 0.73
|             **[C.8.3]** <span style="color: #FF0000">OVWINC</span>: Loading = 0.59, Communality = 0.79
|             **[C.8.4]** Cronbach's Alpha = 0.86
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.6.1, warning=FALSE, message=FALSE}
##################################
# Implementing various procedures for determining
# factor retention based on
# the maximum consensus between methods
##################################
(FA_PA_V_MethodAgreementProcedure <- parameters::n_factors(DPA.Descriptors.Numeric,
                                                           algorithm = "pa",
                                                           rotation = "varimax"))

as.data.frame(FA_PA_V_MethodAgreementProcedure)

##################################
# Conducting exploratory factor analysis
# using Principal Axes extraction
# and Varimax rotation
# with 3 factors
##################################
(FA_PA_V_3F <- fa(DPA.Descriptors.Numeric,
              nfactors = 3,
              fm="pa",
              rotate = "varimax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_PA_V_3F_Summary <- FA_PA_V_3F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_PA_V_3F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_PA_V_3F_Residual <- residuals(FA_PA_V_3F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_PA_V_3F_RMS <- FA_PA_V_3F$rms)

(FA_PA_V_3F_TLI <- FA_PA_V_3F$TLI)

(FA_PA_V_3F_BIC <- FA_PA_V_3F$BIC)

(FA_PA_V_3F_MaxResidual   <- max(abs(FA_PA_V_3F_Residual),na.rm=TRUE))

(FA_PA_V_3F_HighResidual  <- sum(FA_PA_V_3F_Residual>abs(0.05),na.rm=TRUE))

(FA_PA_V_3F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)

(FA_PA_V_3F_HighResidualRate <- FA_PA_V_3F_HighResidual/FA_PA_V_3F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_PA_V_3F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Varimax Rotation : 3 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("STMORT","OVWINC","RDMORT","HDMORT","DBMORT")],
      check.keys=FALSE)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("MEUNDA","ARTINC")],
      check.keys=FALSE)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("SMPREV","PDMORT")],
      check.keys=FALSE)

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_PA_V_3F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "prax",
                                  nfac = 3,
                                  rotation = "varimax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_PA_V_3F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

##################################
# Conducting exploratory factor analysis
# using Principal Axes extraction
# and Varimax rotation
# with 4 factors
##################################
(FA_PA_V_4F <- fa(DPA.Descriptors.Numeric,
              nfactors = 4,
              fm="pa",
              rotate = "varimax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_PA_V_4F_Summary <- FA_PA_V_4F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_PA_V_4F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_PA_V_4F_Residual <- residuals(FA_PA_V_4F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_PA_V_4F_RMS <- FA_PA_V_4F$rms)

(FA_PA_V_4F_TLI <- FA_PA_V_4F$TLI)

(FA_PA_V_4F_BIC <- FA_PA_V_4F$BIC)

(FA_PA_V_4F_MaxResidual   <- max(abs(FA_PA_V_4F_Residual),na.rm=TRUE))

(FA_PA_V_4F_HighResidual  <- sum(FA_PA_V_4F_Residual>abs(0.05),na.rm=TRUE))

(FA_PA_V_4F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)

(FA_PA_V_4F_HighResidualRate <- FA_PA_V_4F_HighResidual/FA_PA_V_4F_TotalResidual)

##################################
# Graphing the factor loading matrices
##################################
fa.diagram(FA_PA_V_4F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Varimax Rotation : 4 Factors",
           cex=0.75)

##################################
# Computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("MEUNDA","ARTINC")],
      check.keys=FALSE)

##################################
# Computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("RDMORT","DBMORT")],
      check.keys=FALSE)

##################################
# Computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("SMPREV","PDMORT")],
      check.keys=FALSE)

##################################
# Computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("STMORT","OVWINC","HDMORT")],
      check.keys=FALSE)

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_PA_V_4F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "prax",
                                  nfac = 4,
                                  rotation = "varimax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_PA_V_4F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

```

</details>

|
|
####  1.3.6.2 Principal Axes Factor Extraction and Promax Rotation
|
| **[A]** Appplying **Principal Axes** factor extraction and **Promax** rotation, an evaluation was conducted using a set of empirical guidelines to determine the optimal number of factors to be retained for exploratory factor analysis. It was determined that:
|      **[A.1]** The choice of 3 factors was supported by the **Bentler**, **CNG**, **t** and **p** methods for determining the optimal number of factors.
|      **[A.2]** The choice of 4 factors was supported by the **Beta** and **Bartlett** methods for determining the optimal number of factors.
|      **[A.3]** 3 or 4 factors would be sufficiently plausible for an optimal balance between comprehensiveness and parsimony. 
|      **[A.4]** To ensure that both under-extraction and over-extraction are assessed, models with 3 and 4 and factors were sequentially evaluated for their interpretability and theoretical meaningfulness.
|
| **[B]** Results for the exploratory factor analysis using a **3-Factor Structure** were as follows:
|      **[B.1]** Standardized Root Mean Square of the Residual = 0.03
|      **[B.2]** Tucker-Lewis Fit Index = 0.98
|      **[B.3]** Bayesian Information Criterion = -33.48
|      **[B.4]** High Residual Rate = 0.11
|      **[B.5]** Factor 1 was a latent factor with higher loading towards the following descriptors:
|             **[B.5.1]** <span style="color: #FF0000">STMORT</span>: Loading = 0.98, Communality = 0.80
|             **[B.5.2]** <span style="color: #FF0000">OVWINC</span>: Loading = 0.73, Communality = 0.72
|             **[B.5.3]** <span style="color: #FF0000">RDMORT</span>: Loading = 0.74, Communality = 0.63
|             **[B.5.4]** <span style="color: #FF0000">HDMORT</span>: Loading = 0.52, Communality = 0.62
|             **[B.5.5]** <span style="color: #FF0000">DBMORT</span>: Loading = 0.48, Communality = 0.41
|             **[B.5.6]** Cronbach's Alpha = 0.88
|      **[B.6]** Factor 2 was a latent factor with higher loading towards the following descriptors:
|             **[B.6.1]** <span style="color: #FF0000">SMPREV</span>: Loading = 0.91, Communality = 0.90
|             **[B.6.2]** <span style="color: #FF0000">PDMORT</span>: Loading = 0.63, Communality = 0.63
|             **[B.6.3]** <span style="color: #FF0000">ARTINC</span>: Loading = 0.57, Communality = 0.84
|             **[B.6.4]** Cronbach's Alpha = 0.89
|      **[B.7]** Factor 3 was a latent factor with higher loading towards the following descriptors:
|             **[B.7.1]** <span style="color: #FF0000">MEUNDA</span>: Loading = 1.05, Communality = 1.06
|             **[B.7.2]** Cronbach's Alpha = 0.85
|
| **[C]** Results for the exploratory factor analysis using a **4-Factor Structure** were as follows:
|      **[C.1]** Standardized Root Mean Square of the Residual = 0.01
|      **[C.2]** Tucker-Lewis Fit Index = 1.08
|      **[C.3]** Bayesian Information Criterion = -21.18
|      **[C.4]** High Residual Rate = 0.05
|      **[C.5]** Factor 1 was a latent factor with higher loading towards the following descriptors:
|             **[C.5.1]** <span style="color: #FF0000">MEUNDA</span>: Loading = 0.91, Communality = 0.90
|             **[C.5.2]** <span style="color: #FF0000">ARTINC</span>: Loading = 0.56, Communality = 0.89
|             **[C.5.3]** Cronbach's Alpha = 0.89
|      **[C.6]** Factor 2 was a latent factor with higher loading towards the following descriptors:
|             **[C.6.1]** <span style="color: #FF0000">RDMORT</span>: Loading = 1.04, Communality = 0.95
|             **[C.6.2]** <span style="color: #FF0000">DBMORT</span>: Loading = 0.47, Communality = 0.44
|             **[C.6.3]** Cronbach's Alpha = 0.76
|      **[C.7]** Factor 3 was a latent factor with higher loading towards the following descriptors:
|             **[C.7.1]** <span style="color: #FF0000">SMPREV</span>: Loading = 0.87, Communality = 0.87
|             **[C.7.2]** <span style="color: #FF0000">PDMORT</span>: Loading = 0.71, Communality = 0.65
|             **[C.7.3]** Cronbach's Alpha = 0.85
|      **[C.8]** Factor 4 was a latent factor with higher loading towards the following descriptors:
|             **[C.8.1]** <span style="color: #FF0000">HDMORT</span>: Loading = 0.78, Communality = 0.79
|             **[C.8.2]** <span style="color: #FF0000">STMORT</span>: Loading = 0.60, Communality = 0.73
|             **[C.8.3]** <span style="color: #FF0000">OVWINC</span>: Loading = 0.53, Communality = 0.79
|             **[C.8.4]** Cronbach's Alpha = 0.86
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.6.2, warning=FALSE, message=FALSE}
##################################
# Implementing various procedures for determining
# factor retention based on
# the maximum consensus between methods
##################################
(FA_PA_P_MethodAgreementProcedure <- parameters::n_factors(DPA.Descriptors.Numeric,
                                                           algorithm = "pa",
                                                           rotation = "promax"))

as.data.frame(FA_PA_P_MethodAgreementProcedure)

##################################
# Conducting exploratory factor analysis
# using Principal Axes extraction
# and Promax rotation
# with 3 factors
##################################
(FA_PA_P_3F <- fa(DPA.Descriptors.Numeric,
              nfactors = 3,
              fm="pa",
              rotate = "promax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_PA_P_3F_Summary <- FA_PA_P_3F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_PA_P_3F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_PA_P_3F_Residual <- residuals(FA_PA_P_3F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_PA_P_3F_RMS <- FA_PA_P_3F$rms)

(FA_PA_P_3F_TLI <- FA_PA_P_3F$TLI)

(FA_PA_P_3F_BIC <- FA_PA_P_3F$BIC)

(FA_PA_P_3F_MaxResidual   <- max(abs(FA_PA_P_3F_Residual),na.rm=TRUE))

(FA_PA_P_3F_HighResidual  <- sum(FA_PA_P_3F_Residual>abs(0.05),na.rm=TRUE))

(FA_PA_P_3F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)

(FA_PA_P_3F_HighResidualRate <- FA_PA_P_3F_HighResidual/FA_PA_P_3F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_PA_P_3F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Promax Rotation : 3 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("STMORT","OVWINC","RDMORT","HDMORT","DBMORT")],
      check.keys=FALSE)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("MEUNDA","ARTINC")],
      check.keys=FALSE)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("SMPREV","PDMORT")],
      check.keys=FALSE)

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_PA_P_3F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "prax",
                                  nfac = 3,
                                  rotation = "promax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_PA_P_3F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

##################################
# Conducting exploratory factor analysis
# using Principal Axes extraction
# and Promax rotation
# with 4 factors
##################################
(FA_PA_P_4F <- fa(DPA.Descriptors.Numeric,
              nfactors = 4,
              fm="pa",
              rotate = "promax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_PA_P_4F_Summary <- FA_PA_P_4F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_PA_P_4F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_PA_P_4F_Residual <- residuals(FA_PA_P_4F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_PA_P_4F_RMS <- FA_PA_P_4F$rms)

(FA_PA_P_4F_TLI <- FA_PA_P_4F$TLI)

(FA_PA_P_4F_BIC <- FA_PA_P_4F$BIC)

(FA_PA_P_4F_MaxResidual   <- max(abs(FA_PA_P_4F_Residual),na.rm=TRUE))

(FA_PA_P_4F_HighResidual  <- sum(FA_PA_P_4F_Residual>abs(0.05),na.rm=TRUE))

(FA_PA_P_4F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)

(FA_PA_P_4F_HighResidualRate <- FA_PA_P_4F_HighResidual/FA_PA_P_4F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_PA_P_4F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Promax Rotation : 4 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("MEUNDA","ARTINC")],
      check.keys=FALSE)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("RDMORT","DBMORT")],
      check.keys=FALSE)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("SMPREV","PDMORT")],
      check.keys=FALSE)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("STMORT","OVWINC","HDMORT")],
      check.keys=FALSE)

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_PA_P_4F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "prax",
                                  nfac = 4,
                                  rotation = "promax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_PA_P_4F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

```

</details>

|
|
####  1.3.6.3 Maximum Likelihood Factor Extraction and Varimax Rotation
|
| **[A]** Appplying **Maximum Likelihood** factor extraction and **Varimax** rotation, an evaluation was conducted using a set of empirical guidelines to determine the optimal number of factors to be retained for exploratory factor analysis. It was determined that:
|      **[A.1]** The choice of 3 factors was supported by the **Bentler**, **CNG**, **t** and **p** methods for determining the optimal number of factors.
|      **[A.2]** The choice of 4 factors was supported by the **Beta**, **Bartlett** and **Adjusted BIC** methods for determining the optimal number of factors.
|      **[A.3]** 3 or 4 factors would be sufficiently plausible for an optimal balance between comprehensiveness and parsimony. 
|      **[A.4]** To ensure that both under-extraction and over-extraction are assessed, models with 3 and 4 and factors were sequentially evaluated for their interpretability and theoretical meaningfulness.
|
| **[B]** Results for the exploratory factor analysis using a **3-Factor Structure** were as follows:
|      **[B.1]** Standardized Root Mean Square of the Residual = 0.03
|      **[B.2]** Tucker-Lewis Fit Index = 0.99
|      **[B.3]** Bayesian Information Criterion = -34.37
|      **[B.4]** High Residual Rate = 0.13
|      **[B.5]** Factor 1 was a latent factor with higher loading towards the following descriptors:
|             **[B.5.1]** <span style="color: #FF0000">STMORT</span>: Loading = 0.81, Communality = 0.79
|             **[B.5.2]** <span style="color: #FF0000">OVWINC</span>: Loading = 0.74, Communality = 0.75
|             **[B.5.3]** <span style="color: #FF0000">RDMORT</span>: Loading = 0.68, Communality = 0.61
|             **[B.5.4]** <span style="color: #FF0000">HDMORT</span>: Loading = 0.60, Communality = 0.67
|             **[B.5.5]** <span style="color: #FF0000">DBMORT</span>: Loading = 0.51, Communality = 0.38
|             **[B.5.6]** Cronbach's Alpha = 0.88
|      **[B.6]** Factor 2 was a latent factor with higher loading towards the following descriptors:
|             **[B.6.1]** <span style="color: #FF0000">MEUNDA</span>: Loading = 0.95, Communality = 1.00
|             **[B.6.2]** <span style="color: #FF0000">ARTINC</span>: Loading = 0.64, Communality = 0.84
|             **[B.6.3]** Cronbach's Alpha = 0.89
|      **[B.7]** Factor 3 was a latent factor with higher loading towards the following descriptors:
|             **[B.7.1]** <span style="color: #FF0000">SMPREV</span>: Loading = 0.79, Communality = 0.93
|             **[B.7.2]** <span style="color: #FF0000">PDMORT</span>: Loading = 0.56, Communality = 0.61
|             **[B.7.3]** Cronbach's Alpha = 0.85
|
| **[C]** Results for the exploratory factor analysis using a **4-Factor Structure** were as follows:
|      **[C.1]** Standardized Root Mean Square of the Residual = 0.01
|      **[C.2]** Tucker-Lewis Fit Index = 1.09
|      **[C.3]** Bayesian Information Criterion = -21.64
|      **[C.4]** High Residual Rate = 0.06
|      **[C.5]** Factor 1 was a latent factor with higher loading towards the following descriptors:
|             **[C.5.1]** <span style="color: #FF0000">MEUNDA</span>: Loading = 0.90, Communality = 0.90
|             **[C.5.2]** <span style="color: #FF0000">ARTINC</span>: Loading = 0.69, Communality = 0.89
|             **[C.5.3]** Cronbach's Alpha = 0.89
|      **[C.6]** Factor 2 was a latent factor with higher loading towards the following descriptors:
|             **[C.6.1]** <span style="color: #FF0000">RDMORT</span>: Loading = 0.90, Communality = 0.96
|             **[C.6.2]** <span style="color: #FF0000">DBMORT</span>: Loading = 0.51, Communality = 0.43
|             **[C.6.3]** Cronbach's Alpha = 0.76
|      **[C.7]** Factor 3 was a latent factor with higher loading towards the following descriptors:
|             **[C.7.1]** <span style="color: #FF0000">SMPREV</span>: Loading = 0.75, Communality = 0.90
|             **[C.7.2]** <span style="color: #FF0000">PDMORT</span>: Loading = 0.60, Communality = 0.63
|             **[C.7.3]** Cronbach's Alpha = 0.85
|      **[C.8]** Factor 4 was a latent factor with higher loading towards the following descriptors:
|             **[C.8.1]** <span style="color: #FF0000">HDMORT</span>: Loading = 0.66, Communality = 0.77
|             **[C.8.2]** <span style="color: #FF0000">STMORT</span>: Loading = 0.60, Communality = 0.73
|             **[C.8.3]** <span style="color: #FF0000">OVWINC</span>: Loading = 0.60, Communality = 0.80
|             **[C.8.4]** Cronbach's Alpha = 0.86
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.6.3, warning=FALSE, message=FALSE}
##################################
# Implementing various procedures for determining
# factor retention based on
# the maximum consensus between methods
##################################
(FA_ML_V_MethodAgreementProcedure <- parameters::n_factors(DPA.Descriptors.Numeric,
                                                           algorithm = "mle",
                                                           rotation = "varimax"))

as.data.frame(FA_ML_V_MethodAgreementProcedure)

##################################
# Conducting exploratory factor analysis
# using Maximum Likelihood extraction
# and Varimax rotation
# with 3 factors
##################################
(FA_ML_V_3F <- fa(DPA.Descriptors.Numeric,
              nfactors = 3,
              fm="ml",
              rotate = "varimax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_ML_V_3F_Summary <- FA_ML_V_3F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_ML_V_3F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_ML_V_3F_Residual <- residuals(FA_ML_V_3F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_ML_V_3F_RMS <- FA_ML_V_3F$rms)

(FA_ML_V_3F_TLI <- FA_ML_V_3F$TLI)

(FA_ML_V_3F_BIC <- FA_ML_V_3F$BIC)

(FA_ML_V_3F_MaxResidual   <- max(abs(FA_ML_V_3F_Residual),na.rm=TRUE))

(FA_ML_V_3F_HighResidual  <- sum(FA_ML_V_3F_Residual>abs(0.05),na.rm=TRUE))

(FA_ML_V_3F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)

(FA_ML_V_3F_HighResidualRate <- FA_ML_V_3F_HighResidual/FA_ML_V_3F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_ML_V_3F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Varimax Rotation : 3 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("STMORT","OVWINC","RDMORT","HDMORT","DBMORT")],
      check.keys=FALSE)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("MEUNDA","ARTINC")],
      check.keys=FALSE)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("SMPREV","PDMORT")],
      check.keys=FALSE)

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_ML_V_3F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "mle",
                                  nfac = 3,
                                  rotation = "varimax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_ML_V_3F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

##################################
# Conducting exploratory factor analysis
# using Maximum Likelihood extraction
# and Varimax rotation
# with 4 factors
##################################
(FA_ML_V_4F <- fa(DPA.Descriptors.Numeric,
              nfactors = 4,
              fm="ml",
              rotate = "varimax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_ML_V_4F_Summary <- FA_ML_V_4F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_ML_V_4F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_ML_V_4F_Residual <- residuals(FA_ML_V_4F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_ML_V_4F_RMS <- FA_ML_V_4F$rms)

(FA_ML_V_4F_TLI <- FA_ML_V_4F$TLI)

(FA_ML_V_4F_BIC <- FA_ML_V_4F$BIC)

(FA_ML_V_4F_MaxResidual   <- max(abs(FA_ML_V_4F_Residual),na.rm=TRUE))

(FA_ML_V_4F_HighResidual  <- sum(FA_ML_V_4F_Residual>abs(0.05),na.rm=TRUE))

(FA_ML_V_4F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)

(FA_ML_V_4F_HighResidualRate <- FA_ML_V_4F_HighResidual/FA_ML_V_4F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_ML_V_4F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Varimax Rotation : 4 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("MEUNDA","ARTINC")],
      check.keys=FALSE)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("RDMORT","DBMORT")],
      check.keys=FALSE)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("SMPREV","PDMORT")],
      check.keys=FALSE)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("STMORT","OVWINC","HDMORT")],
      check.keys=FALSE)

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_ML_V_4F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "mle",
                                  nfac = 4,
                                  rotation = "varimax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_ML_V_4F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

```

</details>

|
|
####  1.3.6.4 Maximum Likelihood Factor Extraction and Promax Rotation
|
| **[A]** Appplying **Maximum Likelihood** factor extraction and **Promax** rotation, an evaluation was conducted using a set of empirical guidelines to determine the optimal number of factors to be retained for exploratory factor analysis. It was determined that:
|      **[A.1]** The choice of 3 factors was supported by the **Bentler**, **CNG**, **t** and **p** methods for determining the optimal number of factors.
|      **[A.2]** The choice of 4 factors was supported by the **Beta**, **Bartlett** and **Adjusted BIC** methods for determining the optimal number of factors.
|      **[A.3]** 3 or 4 factors would be sufficiently plausible for an optimal balance between comprehensiveness and parsimony. 
|      **[A.4]** To ensure that both under-extraction and over-extraction are assessed, models with 3 and 4 and factors were sequentially evaluated for their interpretability and theoretical meaningfulness.
|
| **[B]** Results for the exploratory factor analysis using a **3-Factor Structure** were as follows:
|      **[B.1]** Standardized Root Mean Square of the Residual = 0.03
|      **[B.2]** Tucker-Lewis Fit Index = 0.99
|      **[B.3]** Bayesian Information Criterion = -34.38
|      **[B.4]** High Residual Rate = 0.17
|      **[B.5]** Factor 1 was a latent factor with higher loading towards the following descriptors:
|             **[B.5.1]** <span style="color: #FF0000">STMORT</span>: Loading = 0.96, Communality = 0.79
|             **[B.5.2]** <span style="color: #FF0000">OVWINC</span>: Loading = 0.76, Communality = 0.75
|             **[B.5.3]** <span style="color: #FF0000">RDMORT</span>: Loading = 0.72, Communality = 0.61
|             **[B.5.4]** <span style="color: #FF0000">HDMORT</span>: Loading = 0.58, Communality = 0.67
|             **[B.5.5]** <span style="color: #FF0000">DBMORT</span>: Loading = 0.50, Communality = 0.38
|             **[B.5.6]** Cronbach's Alpha = 0.88
|      **[B.6]** Factor 2 was a latent factor with higher loading towards the following descriptors:
|             **[B.6.1]** <span style="color: #FF0000">SMPREV</span>: Loading = 0.91, Communality = 0.93
|             **[B.6.2]** <span style="color: #FF0000">PDMORT</span>: Loading = 0.56, Communality = 0.61
|             **[B.6.3]** <span style="color: #FF0000">ARTINC</span>: Loading = 0.54, Communality = 0.84
|             **[B.6.4]** Cronbach's Alpha = 0.89
|      **[B.7]** Factor 3 was a latent factor with higher loading towards the following descriptors:
|             **[B.7.1]** <span style="color: #FF0000">MEUNDA</span>: Loading = 1.02, Communality = 1.00
|             **[B.7.2]** Cronbach's Alpha = 0.85
|
| **[C]** Results for the exploratory factor analysis using a **4-Factor Structure** were as follows:
|      **[C.1]** Standardized Root Mean Square of the Residual = 0.01
|      **[C.2]** Tucker-Lewis Fit Index = 1.09
|      **[C.3]** Bayesian Information Criterion = -21.63
|      **[C.4]** High Residual Rate = 0.06
|      **[C.5]** Factor 1 was a latent factor with higher loading towards the following descriptors:
|             **[C.5.1]** <span style="color: #FF0000">MEUNDA</span>: Loading = 0.91, Communality = 0.90
|             **[C.5.2]** <span style="color: #FF0000">ARTINC</span>: Loading = 0.55, Communality = 0.89
|             **[C.5.3]** Cronbach's Alpha = 0.89
|      **[C.6]** Factor 2 was a latent factor with higher loading towards the following descriptors:
|             **[C.6.1]** <span style="color: #FF0000">RDMORT</span>: Loading = 1.05, Communality = 0.96
|             **[C.6.2]** <span style="color: #FF0000">DBMORT</span>: Loading = 0.48, Communality = 0.43
|             **[C.6.3]** Cronbach's Alpha = 0.76
|      **[C.7]** Factor 3 was a latent factor with higher loading towards the following descriptors:
|             **[C.7.1]** <span style="color: #FF0000">SMPREV</span>: Loading = 0.89, Communality = 0.90
|             **[C.7.2]** <span style="color: #FF0000">PDMORT</span>: Loading = 0.67, Communality = 0.63
|             **[C.7.3]** Cronbach's Alpha = 0.85
|      **[C.8]** Factor 4 was a latent factor with higher loading towards the following descriptors:
|             **[C.8.1]** <span style="color: #FF0000">HDMORT</span>: Loading = 0.76, Communality = 0.77
|             **[C.8.2]** <span style="color: #FF0000">STMORT</span>: Loading = 0.61, Communality = 0.73
|             **[C.8.3]** <span style="color: #FF0000">OVWINC</span>: Loading = 0.56, Communality = 0.79
|             **[C.8.4]** Cronbach's Alpha = 0.86
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.6.4, warning=FALSE, message=FALSE}
##################################
# Implementing various procedures for determining
# factor retention based on
# the maximum consensus between methods
##################################
(FA_ML_P_MethodAgreementProcedure <- parameters::n_factors(DPA.Descriptors.Numeric,
                                                           algorithm = "mle",
                                                           rotation = "promax"))

as.data.frame(FA_ML_P_MethodAgreementProcedure)

##################################
# Conducting exploratory factor analysis
# using Maximum Likelihood extraction
# and Promax rotation
# with 3 factors
##################################
(FA_ML_P_3F <- fa(DPA.Descriptors.Numeric,
              nfactors = 3,
              fm="ml",
              rotate = "promax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_ML_P_3F_Summary <- FA_ML_P_3F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_ML_P_3F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_ML_P_3F_Residual <- residuals(FA_ML_P_3F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_ML_P_3F_RMS <- FA_ML_P_3F$rms)

(FA_ML_P_3F_TLI <- FA_ML_P_3F$TLI)

(FA_ML_P_3F_BIC <- FA_ML_P_3F$BIC)

(FA_ML_P_3F_MaxResidual   <- max(abs(FA_ML_P_3F_Residual),na.rm=TRUE))

(FA_ML_P_3F_HighResidual  <- sum(FA_ML_P_3F_Residual>abs(0.05),na.rm=TRUE))

(FA_ML_P_3F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)

(FA_ML_P_3F_HighResidualRate <- FA_ML_P_3F_HighResidual/FA_ML_P_3F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_ML_P_3F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Promax Rotation : 3 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("STMORT","OVWINC","RDMORT","HDMORT","DBMORT")],
      check.keys=FALSE)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("MEUNDA","ARTINC")],
      check.keys=FALSE)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("SMPREV","PDMORT")],
      check.keys=FALSE)

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_ML_P_3F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "mle",
                                  nfac = 3,
                                  rotation = "promax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_ML_P_3F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

##################################
# Conducting exploratory factor analysis
# using Maximum Likelihood extraction
# and Promax rotation
# with 4 factors
##################################
(FA_ML_P_4F <- fa(DPA.Descriptors.Numeric,
              nfactors = 4,
              fm="ml",
              rotate = "promax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_ML_P_4F_Summary <- FA_ML_P_4F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_ML_P_4F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_ML_P_4F_Residual <- residuals(FA_ML_P_4F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_ML_P_4F_RMS <- FA_ML_P_4F$rms)

(FA_ML_P_4F_TLI <- FA_ML_P_4F$TLI)

(FA_ML_P_4F_BIC <- FA_ML_P_4F$BIC)

(FA_ML_P_4F_MaxResidual   <- max(abs(FA_ML_P_4F_Residual),na.rm=TRUE))

(FA_ML_P_4F_HighResidual  <- sum(FA_ML_P_4F_Residual>abs(0.05),na.rm=TRUE))

(FA_ML_P_4F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)

(FA_ML_P_4F_HighResidualRate <- FA_ML_P_4F_HighResidual/FA_ML_P_4F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_ML_P_4F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Promax Rotation : 4 Factors",
           cex=0.75)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("MEUNDA","ARTINC")],
      check.keys=FALSE)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("RDMORT","DBMORT")],
      check.keys=FALSE)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("SMPREV","PDMORT")],
      check.keys=FALSE)

##################################
# computing the internal consistency
# measure of reliability using the
# Cronbach's alpha coefficient
# for each factor
##################################
alpha(DPA.Descriptors.Numeric[,c("STMORT","OVWINC","HDMORT")],
      check.keys=FALSE)

##################################
# Formulating the dandelion plot to
# visualize both factor variances and loadings
# from the factor loading matrices
##################################
FA_ML_P_4F_FactorLoading <- factload(DPA.Descriptors.Numeric,
                                  cormeth = "pearson",
                                  method = "mle",
                                  nfac = 4,
                                  rotation = "promax")

DandelionPlotPalette <- rev(rainbow(100, start = 0, end = 0.2))

dandelion(FA_ML_P_4F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

```

</details>

|
|
####  1.3.6.5 Model Assessment
|
| **[A]** Among candidates, optimal results were obtained for the exploratory factor analysis models applying **Maximum Likelihood Extraction** with both **3-Factor Structure** and **4-Factor Structure** by demonstrating excellent model fit metrics. In addition, the latent variables obtained were contextually meaningful as individual factors in the analysis.
|      **[A.1]** Lowest Standardized Root Mean Square of the Residual
|      **[A.2]** Highest Tucker-Lewis Fit Index
|      **[A.3]** Lowest Bayesian Information Criterion
|      **[A.4]** Lowest High Residual Rate
|
| **[B]** Among the applied **Varimax** and **Promax** rotation methods, **Varimax** rotation was selected for the improved interpretability of the extracted formulated loadings.
|
| **[C]** The selected **4-Factor Model** from the exploratory factor analysis hypothesized the relationship between 4 latent factors and observed chronic disease indicator variables as follows:
|      **[C.1]** Factor 1 was a latent factor describing **Unhealthy Lifestyle** with higher loading towards the following descriptors:
|             **[C.1.1]** <span style="color: #FF0000">HDMORT</span>: Heart Disease Mortality
|             **[C.1.2]** <span style="color: #FF0000">STMORT</span>: Stroke Mortality
|             **[C.1.3]** <span style="color: #FF0000">OVWINC</span>: Overweight Incidence
|      **[C.2]** Factor 2 was a latent factor describing **Physical Inactivity** with higher loading towards the following descriptors:
|             **[C.2.1]** <span style="color: #FF0000">MEUNDA</span>: Mentally Unhealthy Days
|             **[C.2.2]** <span style="color: #FF0000">ARTINC</span>: Arthritis Incidence
|      **[C.3]** Factor 3 was a latent factor describing **Poor Nutrition** with higher loading towards the following descriptors:
|             **[C.3.1]** <span style="color: #FF0000">RDMORT</span>: Renal Disease Mortality
|             **[C.3.2]** <span style="color: #FF0000">DBMORT</span>: Diabetes Mortality
|      **[C.4]** Factor 4 was a latent factor describing **Tobacco Use** with higher loading towards the following descriptors:
|             **[C.4.1]** <span style="color: #FF0000">SMPREV</span>: Smoking Prevalence
|             **[C.4.2]** <span style="color: #FF0000">PDMORT</span>: Pulmonary Disease Mortality
|
| **[D]** The selected **3-Factor Model** from the exploratory factor analysis hypothesized the relationship between 3 latent factors and observed chronic disease indicator variables as follows:
|      **[D.1]** Factor 1 was a latent factor describing **Unhealthy Lifestyle** with higher loading towards the following descriptors:
|             **[D.1.1]** <span style="color: #FF0000">HDMORT</span>: Heart Disease Mortality
|             **[D.1.2]** <span style="color: #FF0000">STMORT</span>: Stroke Mortality
|             **[D.1.3]** <span style="color: #FF0000">OVWINC</span>: Overweight Incidence
|             **[D.1.4]** <span style="color: #FF0000">MEUNDA</span>: Mentally Unhealthy Days
|             **[D.1.5]** <span style="color: #FF0000">ARTINC</span>: Arthritis Incidence
|      **[D.2]** Factor 2 was a latent factor describing **Poor Nutrition** with higher loading towards the following descriptors:
|             **[D.2.1]** <span style="color: #FF0000">RDMORT</span>: Renal Disease Mortality
|             **[D.2.2]** <span style="color: #FF0000">DBMORT</span>: Diabetes Mortality
|      **[D.3]** Factor 3 was a latent factor describing **Tobacco Use** with higher loading towards the following descriptors:
|             **[D.3.1]** <span style="color: #FF0000">SMPREV</span>: Smoking Prevalence
|             **[D.3.2]** <span style="color: #FF0000">PDMORT</span>: Pulmonary Disease Mortality
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.6.5, warning=FALSE, message=FALSE, fig.width=15, fig.height=5}
##################################
# Consolidating the fit indices
# from the exploratory factor analysis
##################################
FA_RMSSummary <- c(FA_PA_V_3F_RMS,
                   FA_PA_V_4F_RMS,
                   FA_PA_P_3F_RMS,
                   FA_PA_P_4F_RMS,
                   FA_ML_V_3F_RMS,
                   FA_ML_V_4F_RMS,
                   FA_ML_P_3F_RMS,
                   FA_ML_P_4F_RMS)

FA_TLISummary <- c(FA_PA_V_3F_TLI,
                   FA_PA_V_4F_TLI,
                   FA_PA_P_3F_TLI,
                   FA_PA_P_4F_TLI,
                   FA_ML_V_3F_TLI,
                   FA_ML_V_4F_TLI,
                   FA_ML_P_3F_TLI,
                   FA_ML_P_4F_TLI)

FA_BICSummary <- c(FA_PA_V_3F_BIC,
                   FA_PA_V_4F_BIC,
                   FA_PA_P_3F_BIC,
                   FA_PA_P_4F_BIC,
                   FA_ML_V_3F_BIC,
                   FA_ML_V_4F_BIC,
                   FA_ML_P_3F_BIC,
                   FA_ML_P_4F_BIC)

FA_HighResidualRateSummary <- c(FA_PA_V_3F_HighResidualRate,
                                FA_PA_V_4F_HighResidualRate,
                                FA_PA_P_3F_HighResidualRate,
                                FA_PA_P_4F_HighResidualRate,
                                FA_ML_V_3F_HighResidualRate,
                                FA_ML_V_4F_HighResidualRate,
                                FA_ML_P_3F_HighResidualRate,
                                FA_ML_P_4F_HighResidualRate)

FA_AlgorithmSummary <- c("3-Factor Principal Axes Extraction + Varimax Rotation",
                         "4-Factor Principal Axes Extraction + Varimax Rotation",
                         "3-Factor Principal Axes Extraction + Promax Rotation",
                         "4-Factor Principal Axes Extraction + Promax Rotation",
                         "3-Factor Maximum Likelihood Extraction + Varimax Rotation",
                         "4-Factor Maximum Likelihood Extraction + Varimax Rotation",
                         "3-Factor Maximum Likelihood Extraction + Promax Rotation",
                         "4-Factor Maximum Likelihood Extraction + Promax Rotation")

FA_Summary <- cbind(FA_RMSSummary,
                    FA_TLISummary,
                    FA_BICSummary,
                    FA_HighResidualRateSummary,
                    FA_AlgorithmSummary)

FA_Summary <- as.data.frame(FA_Summary)
names(FA_Summary) <- c("RMS",
                       "TLI",
                       "BIC",
                       "HighResidualRate",
                       "Algorithm")

FA_Summary$RMS <- as.numeric(as.character(FA_Summary$RMS))
FA_Summary$TLI <- as.numeric(as.character(FA_Summary$TLI))
FA_Summary$BIC <- as.numeric(as.character(FA_Summary$BIC))
FA_Summary$HighResidualRate <- as.numeric(as.character(FA_Summary$HighResidualRate))

FA_Summary$Algorithm <- factor(FA_Summary$Algorithm ,
                                        levels = c("3-Factor Principal Axes Extraction + Varimax Rotation",
                                                   "4-Factor Principal Axes Extraction + Varimax Rotation",
                                                   "3-Factor Principal Axes Extraction + Promax Rotation",
                                                   "4-Factor Principal Axes Extraction + Promax Rotation",
                                                   "3-Factor Maximum Likelihood Extraction + Varimax Rotation",
                                                   "4-Factor Maximum Likelihood Extraction + Varimax Rotation",
                                                   "3-Factor Maximum Likelihood Extraction + Promax Rotation",
                                                   "4-Factor Maximum Likelihood Extraction + Promax Rotation"))

print(FA_Summary, row.names=FALSE)

##################################
# Consolidating all calculated values
# for the Standardized Root Mean Square of the Residual
##################################
(RMS_Plot <- dotplot(Algorithm ~ RMS,
                     data = FA_Summary,
                     main = "Algorithm Comparison : Standardized Root Mean Square of the Residual",
                     ylab = "Algorithm",
                     xlab = "RMSR",
                     auto.key = list(adj = 1),
                     type=c("p", "h"),  
                     origin = 0,
                     alpha = 0.45,
                     pch = 16,
                     cex = 2))

##################################
# Consolidating all calculated values
# for the Tucker-Lewis Fit Index
##################################
(TLI_Plot <- dotplot(Algorithm ~ TLI,
                     data = FA_Summary,
                     main = "Algorithm Comparison : Tucker-Lewis Fit Index",
                     ylab = "Algorithm",
                     xlab = "TLI",
                     auto.key = list(adj = 1),
                     type=c("p", "h"),  
                     origin = 0,
                     alpha = 0.45,
                     pch = 16,
                     cex = 2))

##################################
# Consolidating all calculated values
# for the Bayesian Information Criterion
##################################
(BIC_Plot <- dotplot(Algorithm ~ BIC,
                     data = FA_Summary,
                     main = "Algorithm Comparison : Bayesian Information Criterion",
                     ylab = "Algorithm",
                     xlab = "BIC",
                     auto.key = list(adj = 1),
                     type=c("p", "h"),  
                     origin = 0,
                     alpha = 0.45,
                     pch = 16,
                     cex = 2))

##################################
# Consolidating all calculated values
# for the High Residual Rate
##################################
(HighResidualRate_Plot <- dotplot(Algorithm ~ HighResidualRate,
                     data = FA_Summary,
                     main = "Algorithm Comparison : High Residual Rate ",
                     ylab = "Algorithm",
                     xlab = "High Residual Rate",
                     auto.key = list(adj = 1),
                     type=c("p", "h"),  
                     origin = 0,
                     alpha = 0.45,
                     pch = 16,
                     cex = 2))

##################################
# Plotting the Factor Loading Diagram
# for the optimal EFA models
##################################
fa.diagram(FA_ML_V_4F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Varimax Rotation : 4 Factors",
           cex=0.75)

fa.diagram(FA_ML_V_3F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Maximum Likelihood Factor Extraction + Varimax Rotation : 3 Factors",
           cex=0.75)


##################################
# Plotting the Dandelion Plot
# for the optimal EFA models
##################################
dandelion(FA_ML_V_4F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

dandelion(FA_ML_V_3F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)

```

</details>

###  1.3.7 Model Performance Validation
|
|
####  1.3.7.1 4-Factor Model Confirmatory Factor Analysis
|
| **[A]** Confirmatory factor analysis was conducted to evaluate the derived **4-Factor Model** from the exploratory factor analysis that hypothesized the relationship between 4 latent factors and observed chronic disease indicator variables as follows:
|      **[A.1]** Factor 1 was a latent factor describing **Unhealthy Lifestyle** with higher loading towards the following descriptors:
|             **[A.1.1]** <span style="color: #FF0000">HDMORT</span>: Heart Disease Mortality
|             **[A.1.2]** <span style="color: #FF0000">STMORT</span>: Stroke Mortality
|             **[A.1.3]** <span style="color: #FF0000">OVWINC</span>: Overweight Incidence
|      **[A.2]** Factor 2 was a latent factor describing **Physical Inactivity** with higher loading towards the following descriptors:
|             **[A.2.1]** <span style="color: #FF0000">MEUNDA</span>: Mentally Unhealthy Days
|             **[A.2.2]** <span style="color: #FF0000">ARTINC</span>: Arthritis Incidence
|      **[A.3]** Factor 3 was a latent factor describing **Poor Nutrition** with higher loading towards the following descriptors:
|             **[A.3.1]** <span style="color: #FF0000">RDMORT</span>: Renal Disease Mortality
|             **[A.3.2]** <span style="color: #FF0000">DBMORT</span>: Diabetes Mortality
|      **[A.4]** Factor 4 was a latent factor describing **Tobacco Use** with higher loading towards the following descriptors:
|             **[A.4.1]** <span style="color: #FF0000">SMPREV</span>: Smoking Prevalence
|             **[A.4.2]** <span style="color: #FF0000">PDMORT</span>: Pulmonary Disease Mortality
|
| **[B]** The **4-Factor Model** demonstrated sufficiently good fit based on the model performance metrics as follows:
|      **[B.1]** Comparative Fit Index = 0.96 (Ideal Criterion: >0.90)
|      **[B.2]** Tucker Lewis Index = 0.94 (Ideal Criterion: >0.90)
|      **[B.3]** Standardized Root Mean Square of the Residual = 0.05 (Ideal Criterion: <0.08)
|      **[B.4]** Root Mean Square Error of Approximation = 0.10 (Ideal Criterion: <0.08)
|
| **[C]** All chronic disease indicator loadings for each latent factor were high (>0.65) and all statistically significant.
|
| **[D]** The **4-Factor Model** demonstrated high total reliability at 0.92 and individual latent factor reliability as follows.
|      **[D.1]** **Physical Inactivity**  Cronbach's Alpha = 0.86
|      **[D.2]** **Poor Nutrition** Cronbach's Alpha = 0.76
|      **[D.3]** **Tobacco Use** = 0.85
|      **[D.4]** **Unhealthy Lifestyle** Cronbach's Alpha = 0.89
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.7.1, warning=FALSE, message=FALSE}
##################################
# Specifying the model structure
# for confirmatory factor analysis
##################################
CFA_4F <- 'UNHLIF =~ HDMORT + STMORT + OVWINC
           TOBUSE =~ SMPREV + PDMORT
           PINACT =~ MEUNDA + ARTINC
           PNUTRI =~ RDMORT + DBMORT'

##################################
# Fitting the model
# for confirmatory factor analysis
##################################
CFA_4F_MODEL <- cfa(CFA_4F, data = DPA.Descriptors.Numeric, mimic =c("MPlus"), std.lv = TRUE)

CFA_4F_MODEL_CHANGE <- modindices(CFA_4F_MODEL, sort = TRUE) 
head(CFA_4F_MODEL_CHANGE, 15)

##################################
# Generating a model performance summary
# for confirmatory factor analysis
##################################
summary(CFA_4F_MODEL, fit.measures=TRUE)

##################################
# Testing the null hypothesis
# that the matrix reproduced by the data
# and the specified model are statistically the same
# as the input or analysis matrix
# (Ideal Criterion: P-Value=>0.05)
##################################
fitMeasures(CFA_4F_MODEL, c("chisq", "df", "pvalue"))

##################################
# Measuring the discrepancy between
# the model-based and observed correlation matrices
# (Ideal Criterion: RMSEA<0.08)
##################################
fitMeasures(CFA_4F_MODEL, c("rmsea", "rmsea.ci.lower", "rmsea.ci.upper", "rmsea.pvalue"))

(CFA_4F_MODEL_RMSEA <- fitMeasures(CFA_4F_MODEL, c("rmsea")))

##################################
# Assessing model adequacy by comparing
# the hypothesized model 
# to a baseline model
# (Ideal Criterion: CFI>0.90)
##################################
(CFA_4F_MODEL_CFI <- fitMeasures(CFA_4F_MODEL, c("cfi")))

##################################
# Assessing model adequacy by evaluating
# the improvement in fit of the hypothesized model
# relative to a baseline model
# (Ideal Criterion: TLI>0.90)
##################################
(CFA_4F_MODEL_TLI <- fitMeasures(CFA_4F_MODEL, c("tli")))

##################################
# Measuring the actual differences (discrepancies)
# between the model-based correlations and the actual correlations
# (Ideal Criterion: SRMR<0.08)
##################################
(CFA_4F_MODEL_SRMR <- fitMeasures(CFA_4F_MODEL, c("srmr")))

##################################
# Evaluating correlation residuals 
# using the pairwise coefficients 
# to provide details about potential model misfit
# (Ideal Criterion: Residuals<0.10)
##################################
residuals(CFA_4F_MODEL)$cov

##################################
# Evaluating loading magnitude estimates 
# and fit statistics
# (Ideal Criterion: Loading Estimates>0.50)
# (Ideal Criterion: P-Value<0.05)
##################################
parameterEstimates(CFA_4F_MODEL) %>% 
  filter(op == "=~")

##################################
# Evaluating the measurement reliability
# of the hypothesized model latent factors
# (Ideal Criterion: Cronbach's Alpha>0.70)
##################################
semTools::compRelSEM(CFA_4F_MODEL, tau.eq=T, obs.var=T, return.total=T)

##################################
# Formulating a path diagram to visualize
# the relationships and paths between latent factors
# and observed chronic disease indicators
# from the hypothesized model
##################################
CFA_4F_MODEL_FACTORS <- CFA_4F_MODEL@pta$vnames$lv[[1]]
size <- .65
semPlot::semPaths(CFA_4F_MODEL, latents = CFA_4F_MODEL_FACTORS, whatLabels = "std", layout = "tree2", 
         rotation = 4, style = "lisrel", optimizeLatRes = TRUE, 
         structural = FALSE, layoutSplit = FALSE,
         intercepts = FALSE, residuals = FALSE, 
         curve = 1, curvature = 3, nCharNodes = 8, 
         sizeLat = 11 * size, sizeMan = 11 * size, sizeMan2 = 4 * size, 
         edge.label.cex = 1.2 * size, 
         edge.color = "#000000", edge.label.position = .40)

```

</details>
|
|
####  1.3.7.2 3-Factor Model Confirmatory Factor Analysis
|
| **[A]** Confirmatory factor analysis was conducted to evaluate the derived **3-Factor Model** from the exploratory factor analysis that hypothesized the relationship between 3 latent factors and observed chronic disease indicator variables as follows:
|      **[A.1]** Factor 1 was a latent factor describing **Unhealthy Lifestyle** with higher loading towards the following descriptors:
|             **[A.1.1]** <span style="color: #FF0000">HDMORT</span>: Heart Disease Mortality
|             **[A.1.2]** <span style="color: #FF0000">STMORT</span>: Stroke Mortality
|             **[A.1.3]** <span style="color: #FF0000">OVWINC</span>: Overweight Incidence
|             **[A.1.4]** <span style="color: #FF0000">RDMORT</span>: Renal Disease Mortality
|             **[A.1.5]** <span style="color: #FF0000">DBMORT</span>: Diabetes Mortality
|      **[A.2]** Factor 2 was a latent factor describing **Tobacco Use** with higher loading towards the following descriptors:
|             **[A.2.1]** <span style="color: #FF0000">SMPREV</span>: Smoking Prevalence
|             **[A.2.2]** <span style="color: #FF0000">PDMORT</span>: Pulmonary Disease Mortality
|             **[A.2.3]** <span style="color: #FF0000">ARTINC</span>: Arthritis Incidence
|      **[A.3]** Factor 3 was a latent factor describing **Poor Mental Health** with higher loading towards the following descriptors:
|             **[A.3.1]** <span style="color: #FF0000">MEUNDA</span>: Mentally Unhealthy Days
|
| **[B]** The **3-Factor Model** demonstrated sufficiently good fit based on the model performance metrics as follows:
|      **[B.1]** Comparative Fit Index = 0.91 (Ideal Criterion: >0.90)
|      **[B.2]** Tucker Lewis Index = 0.86 (Ideal Criterion: >0.90)
|      **[B.3]** Standardized Root Mean Square of the Residual = 0.06 (Ideal Criterion: <0.08)
|      **[B.4]** Root Mean Square Error of Approximation = 0.16 (Ideal Criterion: <0.08)
|
| **[C]** All chronic disease indicator loadings for each latent factor were high (>0.60) and all statistically significant.
|
| **[D]** The **3-Factor Model** demonstrated high total reliability at 0.92 and individual latent factor reliability as follows.
|      **[D.1]** **Unhealthy Lifestyle** Cronbach's Alpha = 0.88
|      **[D.2]** **Tobacco Use** = 0.89
|      **[D.3]** **Poor Mental Health** Cronbach's Alpha = Not Applicable
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.7.2, warning=FALSE, message=FALSE}
##################################
# Specifying the model structure
# for confirmatory factor analysis
##################################
CFA_3F <- 'UNHLIF =~ HDMORT + STMORT + OVWINC + RDMORT + DBMORT
           TOBUSE =~ SMPREV + PDMORT + ARTINC
           POMENT =~ MEUNDA'

##################################
# Fitting the model
# for confirmatory factor analysis
##################################
CFA_3F_MODEL <- cfa(CFA_3F, data = DPA.Descriptors.Numeric, mimic =c("MPlus"), std.lv = TRUE)

##################################
# Generating a model performance summary
# for confirmatory factor analysis
##################################
summary(CFA_3F_MODEL, fit.measures = TRUE)

##################################
# Testing the null hypothesis
# that the matrix reproduced by the data
# and the specified model are statistically the same
# as the input or analysis matrix
# (Ideal Criterion: P-Value=>0.05)
##################################
fitMeasures(CFA_3F_MODEL, c("chisq", "df", "pvalue"))

##################################
# Measuring the discrepancy between
# the model-based and observed correlation matrices
# (Ideal Criterion: RMSEA<0.08)
##################################
fitMeasures(CFA_3F_MODEL, c("rmsea", "rmsea.ci.lower", "rmsea.ci.upper", "rmsea.pvalue"))

(CFA_3F_MODEL_RMSEA <- fitMeasures(CFA_3F_MODEL, c("rmsea")))

##################################
# Assessing model adequacy by comparing
# the hypothesized model 
# to a baseline model
# (Ideal Criterion: CFI>0.90)
##################################
(CFA_3F_MODEL_CFI <- fitMeasures(CFA_3F_MODEL, c("cfi")))

##################################
# Assessing model adequacy by evaluating
# the improvement in fit of the hypothesized model
# relative to a baseline model
# (Ideal Criterion: TLI>0.90)
##################################
(CFA_3F_MODEL_TLI <- fitMeasures(CFA_3F_MODEL, c("tli")))

##################################
# Measuring the actual differences (discrepancies)
# between the model-based correlations and the actual correlations
# (Ideal Criterion: SRMR<0.08)
##################################
(CFA_3F_MODEL_SRMR <- fitMeasures(CFA_3F_MODEL, c("srmr")))

##################################
# Evaluating correlation residuals 
# using the pairwise coefficients 
# to provide details about potential model misfit
# (Ideal Criterion: Residuals<0.10)
##################################
residuals(CFA_3F_MODEL)$cov

##################################
# Evaluating loading magnitude estimates 
# and fit statistics
# (Ideal Criterion: Loading Estimates>0.50)
# (Ideal Criterion: P-Value<0.05)
##################################
parameterEstimates(CFA_3F_MODEL) %>% 
  filter(op == "=~")

##################################
# Evaluating the measurement reliability
# of the hypothesized model latent factors
# (Ideal Criterion: Cronbach's Alpha>0.70)
##################################
semTools::compRelSEM(CFA_3F_MODEL, tau.eq=T, obs.var=T, return.total=T)

##################################
# Formulating a path diagram to visualize
# the relationships and paths between latent factors
# and observed chronic disease indicators
# from the hypothesized model
##################################
CFA_3F_MODEL_FACTORS <- CFA_3F_MODEL@pta$vnames$lv[[1]]
size <- .65
semPlot::semPaths(CFA_3F_MODEL, latents = CFA_3F_MODEL_FACTORS, whatLabels = "std", layout = "tree2", 
         rotation = 4, style = "lisrel", optimizeLatRes = TRUE, 
         structural = FALSE, layoutSplit = FALSE,
         intercepts = FALSE, residuals = FALSE, 
         curve = 1, curvature = 3, nCharNodes = 8, 
         sizeLat = 11 * size, sizeMan = 11 * size, sizeMan2 = 4 * size, 
         edge.label.cex = 1.2 * size, 
         edge.color = "#000000", edge.label.position = .40)
```

</details>

###  1.3.8 Model Selection
|
| **[A]** Among candidates, optimal results were obtained for the confirmatory factor analysis model with a **4-Factor Structure** by demonstrating excellent model fit metrics. In addition, the latent variables obtained were contextually meaningful as individual factors in the analysis.
|      **[A.1]** Highest Comparative Fit Index
|      **[A.2]** Highest Tucker-Lewis Fit Index
|      **[A.3]** Lowest Standardized Root Mean Square of the Residual 
|      **[A.4]** Lowest Root Mean Square Error of Approximation
|
| **[B]** A **4-Factor Model** was evaluated as sufficiently plausible to represent the relationship between 4 latent factors and observed chronic disease indicator variables as follows:
|      **[B.1]** An underlying construct corresponding to **Unhealthy Lifestyle** influencing:
|             **[B.1.1]** <span style="color: #FF0000">HDMORT</span>: Heart Disease Mortality
|             **[B.1.2]** <span style="color: #FF0000">STMORT</span>: Stroke Mortality
|             **[B.1.3]** <span style="color: #FF0000">OVWINC</span>: Overweight Incidence
|      **[B.2]** An underlying construct corresponding to **Physical Inactivity** influencing:
|             **[B.2.1]** <span style="color: #FF0000">MEUNDA</span>: Mentally Unhealthy Days
|             **[B.2.2]** <span style="color: #FF0000">ARTINC</span>: Arthritis Incidence
|      **[B.3]** An underlying construct corresponding to **Poor Nutrition** influencing:
|             **[B.3.1]** <span style="color: #FF0000">RDMORT</span>: Renal Disease Mortality
|             **[B.3.2]** <span style="color: #FF0000">DBMORT</span>: Diabetes Mortality
|      **[B.4]** An underlying construct corresponding to **Tobacco Use** influencing:
|             **[B.4.1]** <span style="color: #FF0000">SMPREV</span>: Smoking Prevalence
|             **[B.4.2]** <span style="color: #FF0000">PDMORT</span>: Pulmonary Disease Mortality
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.8, warning=FALSE, message=FALSE, fig.width=15, fig.height=4}
##################################
# Consolidating the fit indices
# from the confirmatory factor analysis
##################################
CFA_RMSEASummary <- c(CFA_4F_MODEL_RMSEA,
                      CFA_3F_MODEL_RMSEA)

CFA_CFISummary <- c(CFA_4F_MODEL_CFI,
                    CFA_3F_MODEL_CFI)

CFA_TLISummary <- c(CFA_4F_MODEL_TLI,
                    CFA_3F_MODEL_TLI)

CFA_SRMRSummary <- c(CFA_4F_MODEL_SRMR,
                     CFA_3F_MODEL_SRMR)

CFA_AlgorithmSummary <- c("4-Factor Confirmatory Factor Analysis",
                          "3-Factor Confirmatory Factor Analysis")

CFA_Summary <- cbind(CFA_RMSEASummary,
                     CFA_CFISummary,
                     CFA_TLISummary,
                     CFA_SRMRSummary,
                     CFA_AlgorithmSummary)

CFA_Summary <- as.data.frame(CFA_Summary)
names(CFA_Summary) <- c("RMSEA",
                       "CFI",
                       "TLI",
                       "SRMR",
                       "Algorithm")

CFA_Summary$RMSEA <- as.numeric(as.character(CFA_Summary$RMSEA))
CFA_Summary$CFI   <- as.numeric(as.character(CFA_Summary$CFI))
CFA_Summary$TLI   <- as.numeric(as.character(CFA_Summary$TLI))
CFA_Summary$SRMR  <- as.numeric(as.character(CFA_Summary$SRMR))

CFA_Summary$Algorithm <- factor(CFA_Summary$Algorithm,
                                levels = c("4-Factor Confirmatory Factor Analysis",
                                           "3-Factor Confirmatory Factor Analysis"))

print(CFA_Summary, row.names=FALSE)

##################################
# Consolidating all calculated values
# for the Root Mean Square Error of Approximation
##################################
(RMSEA_Plot <- dotplot(Algorithm ~ RMSEA,
                       data = CFA_Summary,
                       main = "Algorithm Comparison : Root Mean Square Error of Approximation",
                       ylab = "Algorithm",
                       xlab = "RMSEA",
                       auto.key = list(adj = 1),
                       type=c("p", "h"),  
                       origin = 0,
                       alpha = 0.45,
                       pch = 16,
                       cex = 2))

##################################
# Consolidating all calculated values
# for the Tucker-Lewis Fit Index
##################################
(CFI_Plot <- dotplot(Algorithm ~ CFI,
                     data = CFA_Summary,
                     main = "Algorithm Comparison : Comparative Fit Index",
                     ylab = "Algorithm",
                     xlab = "CFI",
                     auto.key = list(adj = 1),
                     type=c("p", "h"),  
                     origin = 0,
                     alpha = 0.45,
                     pch = 16,
                     cex = 2))

##################################
# Consolidating all calculated values
# for the Tucker-Lewis Fit Index
##################################
(TLI_Plot <- dotplot(Algorithm ~ TLI,
                     data = CFA_Summary,
                     main = "Algorithm Comparison : Tucker-Lewis Fit Index",
                     ylab = "Algorithm",
                     xlab = "TLI",
                     auto.key = list(adj = 1),
                     type=c("p", "h"),  
                     origin = 0,
                     alpha = 0.45,
                     pch = 16,
                     cex = 2))

##################################
# Consolidating all calculated values
# for the Standardized Root Mean Square of the Residual
##################################
(SRMR_Plot <- dotplot(Algorithm ~ SRMR,
                     data = CFA_Summary,
                     main = "Algorithm Comparison : Standardized Root Mean Square of the Residual",
                     ylab = "Algorithm",
                     xlab = "SRMR",
                     auto.key = list(adj = 1),
                     type=c("p", "h"),  
                     origin = 0,
                     alpha = 0.45,
                     pch = 16,
                     cex = 2))

```

</details>

###  1.3.9 Model Presentation
|
|
####  1.3.9.1 Exploratory Factor Analysis Dandelion Plot
|
| **[A]** The formulated dandelion plot for the final **4-Factor Model** from the exploratory factor analysis showed:
|      **[A.1]** Comparable explained variance between the 4 latent factors as presented through the sizes of the star graphs.
|      **[A.2]** Unique list of chronic disease indicators with the most dominant loadings identified in the star graphs representing each latent factor.
|      **[A.3]** Chronic disease indicators with the most and least communality contributions.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.9.1, warning=FALSE, message=FALSE}
##################################
# Plotting the Dandelion Plot
# for the optimal EFA model
##################################
dandelion(FA_ML_V_4F_FactorLoading,
          bound=0,
          mcex=c(1,1.2),
          palet=DandelionPlotPalette)
```

</details>

|
|
####  1.3.9.2 Confirmatory Factor Analysis Path Diagram
|
| **[A]** The formulated path diagram for the final **4-Factor Model** from the confirmatory factor analysis showed:
|      **[A.1]** magnitude and direction of relationships between the underlying constructs and the chronic disease indicators.
|      **[A.2]** estimated variance for each chronic disease indicator.
|      **[A.3]** estimated pairwise covariance between the underlying constructs.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.9.2, warning=FALSE, message=FALSE}
##################################
# Plotting the Path Diagram
# for the optimal CFA model
##################################
CFA_4F_MODEL_FACTORS <- CFA_4F_MODEL@pta$vnames$lv[[1]]
size <- .65
semPlot::semPaths(CFA_4F_MODEL, latents = CFA_4F_MODEL_FACTORS, whatLabels = "std", layout = "tree2", 
         rotation = 4, style = "lisrel", optimizeLatRes = TRUE, 
         structural = FALSE, layoutSplit = FALSE,
         intercepts = FALSE, residuals = FALSE, 
         curve = 1, curvature = 3, nCharNodes = 8, 
         sizeLat = 11 * size, sizeMan = 11 * size, sizeMan2 = 4 * size, 
         edge.label.cex = 1.2 * size, 
         edge.color = "#000000", edge.label.position = .40)
```

</details>
|
|
# **2. Summary** <a name="summary"></a>
|
| ![](D:/Github_Codes/ProjectPortfolio/Portfolio_Project_37/docs/summary_1.png)
|
| ![](D:/Github_Codes/ProjectPortfolio/Portfolio_Project_37/docs/summary_2.png)
|
| ![](D:/Github_Codes/ProjectPortfolio/Portfolio_Project_37/docs/summary_3.png)
|
| ![](D:/Github_Codes/ProjectPortfolio/Portfolio_Project_37/docs/summary_4.png)
|
# **3. References**
|
| **[Book]** [Multivariate Data Analysis](https://prod.cengageasia.com/title/default/detail?isbn=9781473756540) by Barry Babin, Joseph Hair, Rolph Anderson and William Black
| **[Book]** [Using Multivariate Analysis](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) by Barbara Tabachnick and Linda Fidell
| **[Book]** [A Step-by-Step Guide to Exploratory Factor Analysis with R and RStudio](https://www.taylorfrancis.com/books/mono/10.4324/9781003120001/step-step-guide-exploratory-factor-analysis-rstudio-marley-watkins) by Marley Watkins
| **[Book]** [Just Enough R](https://benwhalley.github.io/just-enough-r/) by Ben Whalley
| **[Book]** [Multiple Factor Analysis by Example Using R](https://www.oreilly.com/library/view/multiple-factor-analysis/9781498786690/) by Jerome Pages
| **[Book]** [Nonlinear Principal Component Analysis and Its Applications](https://link.springer.com/book/10.1007/978-981-10-0159-8#toc) by Yuichi Mori, Masahiro Kuroda and Naomichi Makino
| **[Book]** [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) by Max Kuhn and Kjell Johnson
| **[Book]** [An Introduction to Statistical Learning](https://www.statlearning.com/) by Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani
| **[Book]** [Multivariate Data Visualization with R](http://lmdvr.r-forge.r-project.org/figures/figures.html) by Deepayan Sarkar
| **[Book]** [Machine Learning](https://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/) by Samuel Jackson
| **[Book]** [Data Modeling Methods](https://bookdown.org/larget_jacob/data-modeling-methods/) by Jacob Larget
| **[Book]** [Introduction to R and Statistics](https://saestatsteaching.tech/) by University of Western Australia
| **[Book]** [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/index.html) by Max Kuhn and Kjell Johnson
| **[Book]** [Introduction to Research Methods](https://bookdown.org/ejvanholm/Textbook/) by Eric van Holm
| **[Book]** [Using R for Social Work Research](https://bookdown.org/bean_jerry/using_r_for_social_work_research/) by Jerry Bean
| **[Book]** [Introduction to R](https://methodenlehre.github.io/SGSCLM-R-course/index.html#requirements) by Andrew Ellis and Boris Mayer
| **[Book]** [Handbook of Structural Equation Modeling](https://www.guilford.com/books/Handbook-of-Structural-Equation-Modeling/Rick-Hoyle/9781462544646) by Rick Hoyle
| **[Book]** [Applied Multivariate Statistics for the Social Sciences](https://www.taylorfrancis.com/books/edit/10.4324/9780203843130/applied-multivariate-statistics-social-sciences-james-stevens?refId=e8ea9e63-6098-4b67-b405-2413bc50e8fb&context=ubx) by James Stevens
| **[Book]** [Exploratory and Confirmatory Factor Analysis: Understanding Concepts and Applications](https://www.amazon.sg/Exploratory-Confirmatory-Factor-Analysis-Understanding/dp/1591470935) by Bruce Thompson
| **[Book]** [A Beginner's Guide to Structural Equation Modeling](https://www.routledge.com/A-Beginners-Guide-to-Structural-Equation-Modeling/Whittaker-Schumacker/p/book/9780367477967) by Tiffany Whittaker and Randall Schumacker
| **[Book]** [Principles and Practice of Structural Equation Modeling](https://www.guilford.com/books/Principles-and-Practice-of-Structural-Equation-Modeling/Rex-Kline/9781462551910) by Rex Kline
| **[Book]** [Structural Equation Modeling with EQS and EQS/WINDOWS](https://us.sagepub.com/en-us/nam/structural-equation-modeling-with-eqs-and-eqswindows/book4319) by Barbara Byrne 
| **[R Package]** [AppliedPredictiveModeling](https://cran.r-project.org/web//packages/AppliedPredictiveModeling/AppliedPredictiveModeling.pdf) by Max Kuhn
| **[R Package]** [caret](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[R Package]** [rpart](https://mran.microsoft.com/web/packages/rpart/rpart.pdf) by Terry Therneau and Beth Atkinson
| **[R Package]** [lattice](https://cran.r-project.org/web/packages/lattice/lattice.pdf) by  Deepayan Sarkar
| **[R Package]** [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html/) by Hadley Wickham
| **[R Package]** [tidyr](https://cran.r-project.org/web/packages/tidyr/tidyr.pdf) by Hadley Wickham
| **[R Package]** [moments](https://cran.r-project.org/web/packages/moments/index.html) by Lukasz Komsta and Frederick
| **[R Package]** [skimr](https://cran.r-project.org/web/packages/skimr/skimr.pdf) by  Elin Waring
| **[R Package]** [RANN](https://cran.r-project.org/web/packages/RANN/RANN.pdf) by  Sunil Arya, David Mount, Samuel Kemp and Gregory Jefferis
| **[R Package]** [corrplot](https://cran.r-project.org/web/packages/corrplot/corrplot.pdf) by Taiyun Wei
| **[R Package]** [tidyverse](https://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf) by Hadley Wickham
| **[R Package]** [lares](https://cran.rstudio.com/web/packages/lares/lares.pdf) by Bernardo Lares
| **[R Package]** [DMwR](https://mran.microsoft.com/snapshot/2016-05-02/web/packages/DMwR/DMwR.pdf) by Luis Torgo
| **[R Package]** [gridExtra](https://cran.r-project.org/web/packages/gridExtra/gridExtra.pdf) by Baptiste Auguie and Anton Antonov
| **[R Package]** [rattle](https://cran.r-project.org/web/packages/rattle/rattle.pdf) by Graham Williams
| **[R Package]** [RColorBrewer](https://cran.r-project.org/web//packages/RColorBrewer/RColorBrewer.pdf) by Erich Neuwirth
| **[R Package]** [stats](https://search.r-project.org/R/refmans/stats/html/00Index.html) by R Core Team
| **[R Package]** [factoextra](https://cran.r-project.org/web/packages/factoextra/factoextra.pdf) by Alboukadel Kassambara and Fabian Mundt
| **[R Package]** [FactoMineR](https://search.r-project.org/R/refmans/stats/html/00Index.html) by Francois Husson, Julie Josse, Sebastien Le and Jeremy Mazet
| **[R Package]** [gplots](https://cran.r-project.org/web/packages/gplots/gplots.pdf) by Tal Galili
| **[R Package]** [qgraph](https://cran.r-project.org/web/packages/qgraph/qgraph.pdf) by Sacha Epskamp
| **[R Package]** [ggplot2](https://search.r-project.org/R/refmans/stats/html/00Index.html) by Hadley Wickham, Winston Chang, Lionel Henry and Thomas Lin Pedersen
| **[R Package]** [psych](https://cran.r-project.org/web/packages/psych/psych.pdf) by William Revelle
| **[R Package]** [nFactors](https://cran.r-project.org/web/packages/nFactors/nFactors.pdf) by Gilles Raiche and David Magis
| **[R Package]** [MBESS](https://cran.r-project.org/web/packages/MBESS/MBESS.pdf) by Ken Kelley
| **[R Package]** [DandEFA](https://cran.r-project.org/web/packages/DandEFA/DandEFA.pdf) by Artur Manukyan, Ahmet Sedef, Erhan Cene and Ibrahim Demir
| **[R Package]** [EFAtools](https://cran.r-project.org/web/packages/EFAtools/EFAtools.pdf) by Markus Steiner and Silvia Grieder
| **[R Package]** [parameters](https://cran.r-project.org/web/packages/parameters/parameters.pdf) by Daniel Ludecke
| **[R Package]** [performance](https://cran.r-project.org/web/packages/performance/performance.pdf) by Daniel Ludecke
| **[R Package]** [HH](https://cran.r-project.org/web/packages/HH/HH.pdf) by Richard Heiberger
| **[Article]** [6 Dimensionality Reduction Techniques in R (with Examples)](https://cmdlinetips.com/2022/07/dimensionality-reduction-techniques-in-r/) by CMDLineTips Team
| **[Article]** [6 Dimensionality Reduction Algorithms With Python](https://machinelearningmastery.com/dimensionality-reduction-algorithms-with-python/) by Jason Brownlee
| **[Article]** [Introduction to Dimensionality Reduction for Machine Learning](https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/) by Jason Brownlee
| **[Article]** [Introduction to Dimensionality Reduction](https://www.geeksforgeeks.org/dimensionality-reduction/) by Geeks For Geeks
| **[Article]** [Factor Analysis with the psych package](https://m-clark.github.io/posts/2020-04-10-psych-explained/) by Michael Clark
| **[Article]** [Factor Analysis in R with Psych Package: Measuring Consumer Involvement](https://www.r-bloggers.com/2019/01/factor-analysis-in-r-with-psych-package-measuring-consumer-involvement/) by Peter Prevos
| **[Article]** [Factor Analysis in R](http://jinjian-mu.com/tutorial/2021-04-14-Factor%20Analysis/) by Jinjian Mu
| **[Article]** [How To: Use the psych package for Factor Analysis and Data Reduction](http://personality-project.org/r/psych/HowTo/factor.pdf) by William Revelle
| **[Article]** [A Practical Introduction to Factor Analysis: Exploratory Factor Analysis](https://stats.oarc.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/) by UCLA Advanced Research Computing Team
| **[Article]** [Examining the Big 5 Personality Dataset with Factor Analysis](https://taridwong.github.io/posts/2022-01-01-efacfa/) by Tarid Wongvorachan
| **[Article]** [Principal Component Analysis versus Exploratory Factor Analysis](https://support.sas.com/resources/papers/proceedings/proceedings/sugi30/203-30.pdf) by Diana Suhr
| **[Article]** [Exploratory Factor Analysis](https://www.publichealth.columbia.edu/research/population-health-methods/exploratory-factor-analysis) by Columbia University Irving Medical Center
| **[Article]** [Factor Analysis Example](https://real-statistics.com/multivariate-statistics/factor-analysis/factor-analysis-example/) by Charles Zaiontz
| **[Article]** [Factor Analysis Guide with an Example](https://statisticsbyjim.com/basics/factor-analysis/) by Jim Frost
| **[Article]** [What Is Factor Analysis and How Does It Simplify Research Findings?](https://www.qualtrics.com/experience-management/research/factor-analysis/) by Qualtrics Team
| **[Article]** [How Can I Perform A Factor Analysis With Categorical (Or Categorical And Continuous) Variables?](https://stats.oarc.ucla.edu/stata/faq/how-can-i-perform-a-factor-analysis-with-categorical-or-categorical-and-continuous-variables/) by UCLA Advanced Research Computing Team
| **[Article]** [Factor Analysis on Ordinal Data Example in R (psych, homals)](https://alice86.github.io/2018/04/08/Factor-Analysis-on-Ordinal-Data-example-in-R-(psych,-homals)/) by Jiayu Wu
| **[Article]** [Factor Analysis](https://handwiki.org/wiki/Factor%20analysis) by HandWiki Team
| **[Article]** [On Likert Scales In R](https://jakec007.github.io/2021-06-23-R-likert/) by Jake Chanenson
| **[Article]** [Exploratory Factor Analysis](https://opgabriel.github.io/ISMIR2020/visualizations/efa.html) by Mariana Silva
| **[Article]** [Confirmatory Factor Analysis (CFA) in R With LAVAAN](https://stats.oarc.ucla.edu/r/seminars/rcfa/) by UCLA Advanced Research Computing Team
| **[Article]** [Confirmatory Factor Analysis (CFA) Example in R](https://pj.freefaculty.org/guides/crmda_workshops/semexample/R/Ex-02-CFA/cfa-01.html) by Ben Kite
| **[Article]** [Confirmatory Factor Analysis with R](https://shiny.rit.albany.edu/stat/cfa1test/cfabase.pdf) by Bruce Dudek
| **[Article]** [A CFA Example](https://lavaan.ugent.be/tutorial/cfa.html) by Lavaan.Org
| **[RPubs Project]** [EFA and CFA Intro](https://rpubs.com/isbell_daniel/efa_cfa_intro) by Dan Isbell
| **[RPubs Project]** [Factor Analysis: CFA](https://rpubs.com/scblanco/944896) by Silvia Blanco
| **[RPubs Project]** [Exploratory and Confirmatory Factor Analysis in R](https://rpubs.com/Symrna/EFA_CFA) by Ergul Demir
| **[RPubs Project]** [Confirmatory Factor Analysis - CFA](https://rpubs.com/mikhilesh/643303) by Mikhilesh Dehane
| **[RPubs Project]** [Exploratory and Confirmatory Factor Analysis on SF-36 Scales](https://rpubs.com/jbeckste/FA_demo) by Jason Beckstead
| **[RPubs Project]** [Using R for Social Work Research: Exploratory Factor Analysis](https://rpubs.com/JBean/840128) by Jerry Bean
| **[RPubs Project]** [Using R for Social Work Research: Confirmatory Factor Analysis](https://rpubs.com/JBean/723517) by Jerry Bean
| **[Publication]** [General Intelligence Objectively Determined and Measured](https://psycnet.apa.org/record/1926-00296-001) by Charles Spearman (The American Journal of Psychology)
| **[Publication]** [The Effect of Standardization on a Chi-Square Approximation in Factor Analysis](https://www.semanticscholar.org/paper/THE-EFFECT-OF-STANDARDIZATION-ON-A-%CF%872-APPROXIMATION-Bartlett/95d549d2c055360b34cc7d1fce739179c29e39bb) by Maurice Bartlett (Psychometrika)
| **[Publication]** [A Second Generation Little Jiffy](https://link.springer.com/article/10.1007/BF02291817) by Henry Kaiser (Psychometrika)
| **[Publication]** [Tests of Significance in Factor Analysis](https://www.semanticscholar.org/paper/TESTS-OF-SIGNIFICANCE-IN-FACTOR-ANALYSIS-Burt/25975f19d17ab2577845ec3d61f52806b28d8f28) by Maurice Bartlett (British Journal of Statistical Psychology)
| **[Publication]** [Test of Linear Trend in Eigenvalues of a Covariance Matrix with Application to Data Analysis](https://psycnet.apa.org/record/1996-01978-006) by Peter Bentler and KeHai Yuan (British Journal of Mathematical and Statistical Psychology)
| **[Publication]** [The Scree Test For The Number Of Factors](https://www.semanticscholar.org/paper/The-Scree-Test-For-The-Number-Of-Factors.-Cattell/379df72de684003963f11427c97490a8c2d2a593) by Raymond Cattell (Multivariate Behavioral Research)
| **[Publication]** [Using Fit Statistic Differences to Determine the Optimal Number of Factors to Retain in an Exploratory Factor Analysis](https://journals.sagepub.com/doi/10.1177/0013164419865769) by William Finch (Educational and Psychological Measurement)
| **[Publication]** [An Objective Counterpart to the Visual Scree Test for Factor Analysis: The Standard Error Scree](https://journals.sagepub.com/doi/10.1177/0013164496056003006) by Keith Zoski and Stephen Jurs (Educational and Psychological Measurement)
| **[Publication]** [The Performance of Regression-Based Variations of the Visual Scree for Determining the Number of Common Factors](https://journals.sagepub.com/doi/10.1177/00164402062003001) by Fadia Nasser, Jeri Benson and Joseph Wisenbaker (Educational and Psychological Measurement)
| **[Publication]** [Investigating the Performance of Exploratory Graph Analysis and Traditional Techniques to Identify the Number of Latent Factors: A Simulation and Tutorial](https://www.semanticscholar.org/paper/Investigating-the-performance-of-exploratory-graph-Golino-Shi/470c4e8aeebd08699fe9092463540a1b24b7e2e8) by Hudson Golino, Dingjing Shi, Alexander Christensen, Luis Garrido, Maria Nieto, Ritu Sadana, Jotheeswaran Thiyagarajan, Agustin Martinez-Molina (Psychological Methods)
| **[Publication]** [Exploratory Graph Analysis: A New Approach for Estimating the Number of Dimensions in Psychological Research](https://www.semanticscholar.org/paper/Exploratory-graph-analysis%3A-A-new-approach-for-the-Golino-Epskamp/f44110bff4345eb228b27de8a0b8aec235edd478) by Hudson Galino and Sacha Epskamp (Plos One)
| **[Publication]** [Very Simple Structure: An Alternative Procedure For Estimating The Optimal Number Of Interpretable Factors](https://www.tandfonline.com/doi/abs/10.1207/s15327906mbr1404_2) by William Revelle and Thomas Rocklin (Multivariate Behavioral Research )
| **[Publication]** [Determining the Number of Components from the Matrix of Partial Correlations](https://psycnet.apa.org/record/1977-07293-001) by Wayne Velicer (Psychometrika)
| **[Publication]** [Dandelion Plot: A Method for the Visualization of R-mode Exploratory Factor Analyses](https://link.springer.com/article/10.1007/s00180-014-0518-x) by Artur Manukyan, Erhan Cene, Ahmet Sedef and Ibrahim Demir (Computational Statistics) 
| **[Publication]** [Cutoff Criteria for Fit Indexes in Covariance Structure Analysis: Conventional Criteria Versus New Alternatives](https://www.tandfonline.com/doi/abs/10.1080/10705519909540118) by Litze Hu  and Peter Bentler (Structural Equation Modeling: A Multidisciplinary Journal)
| **[Publication]** [Effects of Sample Size, Estimation Methods, and Model Specification on Structural Equation Modeling Fit Indexes](https://www.tandfonline.com/doi/abs/10.1080/10705519909540119) by Xitao Fan, Bruce Thompson and Lin Wang (Structural Equation Modeling: A Multidisciplinary Journal)
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
|
|
|
|