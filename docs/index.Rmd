---
title: 'Unsupervised Learning : Uncovering Underlying Constructs of Chronic Disease Indicators Across US States Using Exploratory and Confirmatory Factor Analyses'
author: "John Pauline Pineda"
date: "November 5, 2023"
output: 
  html_document:
    toc: true
    toc_depth: 4
    theme: readable
    highlight: tango
    css: doc.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=15, fig.height=10)
```
# **1. Table of Contents**
|
##  1.1 Introduction
|
| Chronic disease indicators (CDI) are a set of surveillance indicators developed by consensus among the [Center for Disease Controls and Prevention (CDC)](https://www.cdc.gov/), [Council of State and Territorial Epidemiologists (CSTE)](https://www.cste.org/), and [National Association of Chronic Disease Directors (NACDD)](https://chronicdisease.org/). [CDI enables public health professionals and policymakers to retrieve uniformly defined state-level data for chronic diseases and risk factors](https://www.cdc.gov/cdi/overview.html) that have a substantial impact on public health. These indicators are essential for surveillance, prioritization, and evaluation of public health interventions.
|
| Using an open dataset from [Data.World](https://data.world/) as primarily sourced from [CDC](https://www.cdc.gov/cdi/index.html), this study hypothesized that latent patterns are present among sufficiently correlated [indicators](https://datatopics.worldbank.org/world-development-indicators/) encompassing multiple chronic disease topic areas in the US. A number of factor analysis models was formulated to explore and verify the relationship between these factors.
|
| Subsequent analysis and modelling steps involving data understanding, data preparation, data exploration, model development, model validation and model presentation were individually detailed below, with all the results consolidated in a [<span style="color: #FF0000">**Summary**</span>](#summary) provided at the end of the document.
|
###  1.1.1 Study Objectives
|
| **The main objective of the study is to explore and verify the possible underlying factor structures and relationships among a set of observed US chronic disease indicators by identifying latent factors that explain the observed correlations and reducing the complexity of the data by grouping related variables together under these latent factors.**
|
| Specific objectives are given as follows:
|
| **[A]** Obtain an optimal subset of observations and descriptors by conducting data quality assessment and applying preprocessing operations most suitable for the downstream analysis
|
| **[B]** Develop exploratory factor analysis models using various extraction methods to estimate and identify potential underlying structures from observed descriptors and different rotation approaches in simplifying the derived factor structures to achieve a more interpretable pattern of factor loadings
|
| **[C]** Select the final exploratory factor analysis model among candidates based on robust performance estimates
|
| **[D]** Verify the relationship obtained from the final exploratory analysis model between the observed descriptors and their underlying latent constructs using confirmatory factor analysis.
|
###  1.1.2 Outcome
|
| There is no explicit endpoint for the study given the unsupervised learning nature of the analysis.
|
###  1.1.3 Predictors
|
| Detailed descriptions for each individual descriptor used in the study are provided as follows:
|
| **[A]** **Chronic Disease Topic Area : Alcohol**
|      **[A.1]** <span style="color: #FF0000">BDPREV</span> (numeric): **Binge Drinking Prevalence**; Binge drinking prevalence among adults aged >= 18 years.
|      **[A.2]** <span style="color: #FF0000">LDMORT</span> (numeric): **Liver Disease Mortality**; Chronic liver disease mortality.
|
| **[B]** **Chronic Disease Topic Area : Arthritis**
|      **[B.1]** <span style="color: #FF0000">ARTINC</span> (numeric): **Arthritis Incidence**; Arthritis among adults aged >= 18 years.
|
| **[C]** **Chronic Disease Topic Area : Asthma**
|      **[C.1]** <span style="color: #FF0000">ASPREV</span> (numeric): **Asthma Prevalence**; Current asthma prevalence among adults aged >= 18 years.
|      **[C.2]** <span style="color: #FF0000">ASMORT</span> (numeric): **Asthma Mortality**; Asthma mortality rate.
|
| **[D]** **Chronic Disease Topic Area : Cancer**
|      **[D.1]** <span style="color: #FF0000">MAMUSE</span> (numeric): **Mammography Use**; Mammography use among women aged 50-74 years.
|      **[D.2]** <span style="color: #FF0000">PSMUSE</span> (numeric): **Papanicolaou Smear Use**; Papanicolaou smear use among adult women aged 21-65 years.
|      **[D.3]** <span style="color: #FF0000">CCTUSE</span> (numeric): **Colon Cancer Test Use**; Fecal occult blood test, sigmoidoscopy, or colonoscopy among adults aged 50-75 years.
|
| **[E]** **Chronic Disease Topic Area : Chronic Kidney Disease**
|      **[E.1]** <span style="color: #FF0000">RDMORT</span> (numeric): **Renal Disease Mortality**; Mortality with end-stage renal disease.
|      **[E.2]** <span style="color: #FF0000">KDPREV</span> (numeric): **Kidney Disease Prevalence**; Prevalence of chronic kidney disease among adults aged >= 18 years.
|
| **[F]** **Chronic Disease Topic Area : Chronic Obstructive Pulmonary Disease**
|      **[F.1]** <span style="color: #FF0000">PDMORT</span> (numeric): **Pulmonary Disease Mortality**; Mortality with chronic obstructive pulmonary disease as underlying cause among adults aged >=45 years.
|      **[F.2]** <span style="color: #FF0000">PDPREV</span> (numeric): **Pulmonary Disease Prevalence**; Prevalence of chronic obstructive pulmonary disease among adults >= 18.
|      **[F.3]** <span style="color: #FF0000">SCPREV</span> (numeric): **Smoking Prevalence**; Prevalence of current smoking among adults >= 18 with diagnosed chronic obstructive pulmonary disease.
|
| **[G]** **Chronic Disease Topic Area : Cardiovascular Disease**
|      **[G.1]** <span style="color: #FF0000">CDMORT</span> (numeric): **Cardiovascular Disease Mortality**; Mortality from total cardiovascular disease.
|      **[G.2]** <span style="color: #FF0000">HDMORT</span> (numeric): **Heart Disease Mortality**; Mortality from diseases of the heart.
|      **[G.3]** <span style="color: #FF0000">STMORT</span> (numeric): **Stroke Mortality**; Mortality from stroke.
|
| **[H]** **Chronic Disease Topic Area : Diabetes**
|      **[H.1]** <span style="color: #FF0000">DBMORT</span> (numeric): **Diabetes Mortality**; Mortality due to diabetes reported as any listed cause of death.
|      **[H.2]** <span style="color: #FF0000">DBPREV</span> (numeric): **Diabetes Prevalence**; Prevalence of diagnosed diabetes among adults aged >= 18 years.
|
| **[I]** **Chronic Disease Topic Area : Immunization**
|      **[I.1]** <span style="color: #FF0000">INFVAC</span> (numeric): **Influenza Vaccination**; Influenza vaccination among noninstitutionalized adults aged >= 18 years.
|
| **[J]** **Chronic Disease Topic Area : Mental Health**
|      **[I.1]** <span style="color: #FF0000">MEUNDA</span> (numeric): **Mentally Unhealthy Days**; Recent mentally unhealthy days among adults aged >= 18 years.
|
| **[K]** **Chronic Disease Topic Area : Nutrition, Physical Activity and Weight Status**
|      **[K.1]** <span style="color: #FF0000">PHYACT</span> (numeric): **Physical Activity**; No leisure-time physical activity among adults aged >= 18 years.
|      **[K.2]** <span style="color: #FF0000">OVWINC</span> (numeric): **Overweight Incidence**; Overweight or obesity among adults aged >= 18 years.
|
| **[L]** **Chronic Disease Topic Area : Oral Health**
|      **[L.1]** <span style="color: #FF0000">DENVIS</span> (numeric): **Dentist Visit**; Visits to dentist or dental clinic among adults aged >= 18 years.
|      **[L.2]** <span style="color: #FF0000">TTHLOS</span> (numeric): **Tooth Loss**; No tooth loss among adults aged 18-64 years.
|
| **[M]** **Chronic Disease Topic Area : Overarching Conditions**
|      **[M.1]** <span style="color: #FF0000">HLTINS</span> (numeric): **Health Insurance**; Current lack of health insurance among adults aged 18-64 years.
|      **[M.2]** <span style="color: #FF0000">HLTSTA</span> (numeric): **Health Status**; Fair or poor self-rated health status among adults aged >= 18 years.
|      **[M.3]** <span style="color: #FF0000">SLPSTA</span> (numeric): **Sleep Status**; Prevalence of sufficient sleep among adults aged >= 18 years.
|
| **[N]** **Chronic Disease Topic Area : Tobacco**
|      **[N.1]** <span style="color: #FF0000">SMPREV</span> (numeric): **Smoking Prevalence**; Current smoking among adults aged >= 18 years.
|      **[N.2]** <span style="color: #FF0000">PNEVAC</span> (numeric): **Pneumococcal Vaccination**; Pneumococcal vaccination among noninstitutionalized adults aged 18-64 years who smoke.
|      **[N.3]** <span style="color: #FF0000">QUITAT</span> (numeric): **Quit Attempts**; Quit attempts in the past year among current smokers.
|
##  1.2 Methodology
|
###  1.2.1 Data Assessment
|
| Preliminary data used in the study was evaluated and prepared for analysis and modelling using the following methods: 
|
| [Covariance Validity](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) evaluates whether the ratio of associated variables in the data set are sufficient enough to support the assumption that correlations exist. The criterion is computed by determining the proportion of correlation coefficients with values of at least 30% between all pairs of variables in the data set. A value closer to 1 suggests an adequate percentage of pairwise-correlated variables.
|
| [Determinant Computation](https://www.taylorfrancis.com/books/mono/10.4324/9781003120001/step-step-guide-exploratory-factor-analysis-rstudio-marley-watkins) reflects the extent of multicollinearity among the variables in a correlation matrix. An extremely small determinant value indicates that the variables are highly correlated and nearly linearly dependent which can lead to unstable results and difficulty in interpreting their individual contributions.
|
| [Bartlett’s Test of Sphericity](https://www.semanticscholar.org/paper/THE-EFFECT-OF-STANDARDIZATION-ON-A-%CF%872-APPROXIMATION-Bartlett/95d549d2c055360b34cc7d1fce739179c29e39bb) evaluates whether the correlations between variables in a data set are significant enough to support the assumption that underlying factors exist and can be extracted. The test calculates a Chi-Square statistic based on the differences between the observed correlation matrix and an identity matrix. The larger the Chi-Square value, there is more evidence against the null hypothesis stating that the correlation matrix is an identity matrix which indicates that the variables are uncorrelated and do not have any underlying structure.
|
| [Kaiser-Meyer-Olkin Factor Adequacy](https://link.springer.com/article/10.1007/BF02291817) evaluates whether the observed variables are suitable for exploratory factor analysis based on their common variance and the potential for extracting meaningful factors. The criterion is computed by examining the ratio of the sum of squared correlations between variables to the sum of squared partial correlations. A KMO value closer to 1 suggests that the variables have high shared variance and are suitable to proceed with the analysis.
|
###  1.2.2 Model Formulation
|
| [Principal Axes Factor Extraction](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) identifies the underlying constructs that explain the observed correlations among variables by capturing both common variance (shared among variables) and unique variance (specific to each variable). This process potentially results to factors with lower communalities (explained variance) but with more direct interpretability. The algorithm performs eigenvalue decomposition on the correlation matrix. The eigenvalues represent the amount of variance explained by each eigenvector. Given a defined number of factors, loadings are calculated for each observed variable on each extracted factor. Factor loadings indicate the strength and direction of the relationship between variables and factors. Factors are interpreted based on the loading patterns. Variables with high loadings on a factor are strongly associated with the factor.
|
| [Maximum Likelihood Factor Extraction](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) aims to estimate the factor loadings in a way that maximizes the likelihood of observing the given data, assuming a specific factor model. Given the correlation matrix, the algorithm formulates a likelihood function that represents the probability of observing the given data under an assumed factor model representing the relationships between the latent factors and observed variables. The likelihood function quantifies how well the model explains the observed data. Optimization techniques are applied to determine the factor loadings that maximize the likelihood function. The process involves iteratively adjusting the factor loadings to improve the fit between the model and the data. Factor loadings indicate the strength and direction of the relationship between variables and factors. Factors are interpreted based on the loading patterns. Variables with high loadings on a factor are strongly associated with the factor.
|
| [Varimax Rotation](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) is an orthogonal rotation method which forces the rotated factors to be uncorrelated with each other, leading to simpler and more easily interpretable factor solutions. The algorithm aims to maximize the variance of the squared loadings within each factor which helps identify variables that are strongly associated with a single factor. The results are straightforward to interpret and can be particularly useful when the factors are expected to be independent.
|
| [Promax Rotation](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) is an oblique rotation method which allows for more flexibility by accommodating the possibility of correlated factors. The algorithm aims to simplify the factor structure by both maximizing the variance of the squared loadings within each factor and allowing for correlated factors. It uses a more complex mathematical approach to find the optimal rotation that accounts for both variance and correlation. The results provide a more accurate representation of the underlying relationships when the factors are expected to be correlated.
|
|
##  1.3 Results
|
###  1.3.1 Data Preparation
|
| **[A]** The initial tabular dataset was comprised of 50 observations and 31 variables (including 1 metadata and 30 descriptors). 
|      **[A.1]** 50 rows (observations)
|      **[A.2]** 31 columns (variables)
|             **[A.2.1]** 1/31 instance labels = <span style="color: #FF0000">STATE</span> (character)
|             **[A.2.2]** 30/31 predictors = 30/30 numeric
|                      **[A.2.2.1]** <span style="color: #FF0000">BDPREV</span> (numeric)
|                      **[A.2.2.2]** <span style="color: #FF0000">LDMORT</span> (numeric)
|                      **[A.2.2.3]** <span style="color: #FF0000">ARTINC</span> (numeric)
|                      **[A.2.2.4]** <span style="color: #FF0000">ASPREV</span> (numeric)
|                      **[A.2.2.5]** <span style="color: #FF0000">ASMORT</span> (numeric)
|                      **[A.2.2.6]** <span style="color: #FF0000">MAMUSE</span> (numeric)
|                      **[A.2.2.7]** <span style="color: #FF0000">PSMUSE</span> (numeric)
|                      **[A.2.2.8]** <span style="color: #FF0000">CCTUSE</span> (numeric)
|                      **[A.2.2.9]** <span style="color: #FF0000">RDMORT</span> (numeric)
|                      **[A.2.2.10]** <span style="color: #FF0000">KDPREV</span> (numeric)
|                      **[A.2.2.11]** <span style="color: #FF0000">PDMORT</span> (numeric)
|                      **[A.2.2.12]** <span style="color: #FF0000">PDPREV</span> (numeric)
|                      **[A.2.2.13]** <span style="color: #FF0000">SMPREV</span> (numeric)
|                      **[A.2.2.14]** <span style="color: #FF0000">CDMORT</span> (numeric)
|                      **[A.2.2.15]** <span style="color: #FF0000">HDMORT</span> (numeric)
|                      **[A.2.2.16]** <span style="color: #FF0000">STMORT</span> (numeric)
|                      **[A.2.2.17]** <span style="color: #FF0000">DBMORT</span> (numeric)
|                      **[A.2.2.18]** <span style="color: #FF0000">DBPREV</span> (numeric)
|                      **[A.2.2.19]** <span style="color: #FF0000">INFVAC</span> (numeric)
|                      **[A.2.2.20]** <span style="color: #FF0000">MEUNDA</span> (numeric)
|                      **[A.2.2.21]** <span style="color: #FF0000">PHYACT</span> (numeric)
|                      **[A.2.2.22]** <span style="color: #FF0000">OVWINC</span> (numeric)
|                      **[A.2.2.23]** <span style="color: #FF0000">DENVIS</span> (numeric)
|                      **[A.2.2.24]** <span style="color: #FF0000">TTHLOS</span> (numeric)
|                      **[A.2.2.25]** <span style="color: #FF0000">HLTINS</span> (numeric)
|                      **[A.2.2.26]** <span style="color: #FF0000">HLTSTA</span> (numeric)
|                      **[A.2.2.27]** <span style="color: #FF0000">SLPSTA</span> (numeric)
|                      **[A.2.2.28]** <span style="color: #FF0000">SMPREV</span> (numeric)
|                      **[A.2.2.29]** <span style="color: #FF0000">PNEVAC</span> (numeric)
|                      **[A.2.2.30]** <span style="color: #FF0000">QUITAT</span> (numeric)
|
| 

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.1, warning=FALSE, message=FALSE}
##################################
# Loading R libraries
##################################
library(AppliedPredictiveModeling)
library(performance)
library(parameters)
library(HH)
library(tidyr)
library(caret)
library(psych)
library(lattice)
library(dplyr)
library(moments)
library(skimr)
library(RANN)
library(pls)
library(corrplot)
library(lares)
library(DMwR)
library(gridExtra)
library(rattle)
library(RColorBrewer)
library(stats)
library(factoextra)
library(FactoMineR)
library(gplots)
library(qgraph)
library(ggplot2)
library(psych)
library(nFactors)
library(MBESS)
library(mice)
library(DandEFA)
library(EFAtools)

##################################
# Loading source and
# formulating the analysis set
##################################
CDI <- read.csv("ChronicDiseaseIndicators.csv",
                      na.strings=c("NA","NaN"," ",""),
                      stringsAsFactors = FALSE)
CDI <- as.data.frame(CDI)

##################################
# Performing a general exploration of the data set
##################################
dim(CDI)
str(CDI)
summary(CDI)

##################################
# Formulating a data type assessment summary
##################################
PDA <- CDI
(PDA.Summary <- data.frame(
  Column.Index=c(1:length(names(PDA))),
  Column.Name= names(PDA), 
  Column.Type=sapply(PDA, function(x) class(x)), 
  row.names=NULL)
)

```

</details>

###  1.3.2 Data Quality Assessment
|
| **[A]** Missing data noted for 1 variable with Null.Count>0 and Fill.Rate<1.0.
|      **[A.1]** <span style="color: #FF0000">ASMORT</span>: Null.Count=11, Fill.Rate=0.78
| **[B]** No low variance observed for any variable with First.Second.Mode.Ratio>5.
| **[C]** No low variance observed for any variable with Unique.Count.Ratio<0.01.
| **[D]** No high skewness observed for any variable with Skewness>3 or Skewness<(-3).
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.2, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DQA <- CDI

##################################
# Formulating an overall data quality assessment summary
##################################
(DQA.Summary <- data.frame(
  Column.Name= names(DQA),
  Column.Type=sapply(DQA, function(x) class(x)),
  Row.Count=sapply(DQA, function(x) nrow(DQA)),
  NA.Count=sapply(DQA,function(x)sum(is.na(x))),
  Fill.Rate=sapply(DQA,function(x)format(round((sum(!is.na(x))/nrow(DQA)),3),nsmall=3)),
  row.names=NULL)
)

##################################
# Listing all descriptors
##################################
DQA.Descriptors <- DQA[,colnames(DQA)!="State"]

##################################
# Listing all numeric Descriptors
##################################
DQA.Descriptors.Numeric <- DQA.Descriptors[,sapply(DQA.Descriptors, is.numeric)]

if (length(names(DQA.Descriptors.Numeric))>0) {
    print(paste0("There are ",
               (length(names(DQA.Descriptors.Numeric))),
               " numeric descriptor variable(s)."))
} else {
  print("There are no numeric descriptor variables.")
}

##################################
# Listing all factor Descriptors
##################################
DQA.Descriptors.Factor <- DQA.Descriptors[,sapply(DQA.Descriptors, is.factor)]

if (length(names(DQA.Descriptors.Factor))>0) {
    print(paste0("There are ",
               (length(names(DQA.Descriptors.Factor))),
               " factor descriptor variable(s)."))
} else {
  print("There are no factor descriptor variables.")
}

##################################
# Formulating a data quality assessment summary for factor Descriptors
##################################
if (length(names(DQA.Descriptors.Factor))>0) {

  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = x[!(x %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return("x"),
           return(usm[tabsm == max(tabsm)]))
  }

  (DQA.Descriptors.Factor.Summary <- data.frame(
  Column.Name= names(DQA.Descriptors.Factor),
  Column.Type=sapply(DQA.Descriptors.Factor, function(x) class(x)),
  Unique.Count=sapply(DQA.Descriptors.Factor, function(x) length(unique(x))),
  First.Mode.Value=sapply(DQA.Descriptors.Factor, function(x) as.character(FirstModes(x)[1])),
  Second.Mode.Value=sapply(DQA.Descriptors.Factor, function(x) as.character(SecondModes(x)[1])),
  First.Mode.Count=sapply(DQA.Descriptors.Factor, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Descriptors.Factor, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  Unique.Count.Ratio=sapply(DQA.Descriptors.Factor, function(x) format(round((length(unique(x))/nrow(DQA.Descriptors.Factor)),3), nsmall=3)),
  First.Second.Mode.Ratio=sapply(DQA.Descriptors.Factor, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  row.names=NULL)
  )

}

##################################
# Formulating a data quality assessment summary for numeric Descriptors
##################################
if (length(names(DQA.Descriptors.Numeric))>0) {

  ##################################
  # Formulating a function to determine the first mode
  ##################################
  FirstModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    ux[tab == max(tab)]
  }

  ##################################
  # Formulating a function to determine the second mode
  ##################################
  SecondModes <- function(x) {
    ux <- unique(na.omit(x))
    tab <- tabulate(match(x, ux))
    fm = ux[tab == max(tab)]
    sm = na.omit(x)[!(na.omit(x) %in% fm)]
    usm <- unique(sm)
    tabsm <- tabulate(match(sm, usm))
    ifelse(is.na(usm[tabsm == max(tabsm)])==TRUE,
           return(0.00001),
           return(usm[tabsm == max(tabsm)]))
  }

  (DQA.Descriptors.Numeric.Summary <- data.frame(
  Column.Name= names(DQA.Descriptors.Numeric),
  Column.Type=sapply(DQA.Descriptors.Numeric, function(x) class(x)),
  Unique.Count=sapply(DQA.Descriptors.Numeric, function(x) length(unique(x))),
  Unique.Count.Ratio=sapply(DQA.Descriptors.Numeric, function(x) format(round((length(unique(x))/nrow(DQA.Descriptors.Numeric)),3), nsmall=3)),
  First.Mode.Value=sapply(DQA.Descriptors.Numeric, function(x) format(round((FirstModes(x)[1]),3),nsmall=3)),
  Second.Mode.Value=sapply(DQA.Descriptors.Numeric, function(x) format(round((SecondModes(x)[1]),3),nsmall=3)),
  First.Mode.Count=sapply(DQA.Descriptors.Numeric, function(x) sum(na.omit(x) == FirstModes(x)[1])),
  Second.Mode.Count=sapply(DQA.Descriptors.Numeric, function(x) sum(na.omit(x) == SecondModes(x)[1])),
  First.Second.Mode.Ratio=sapply(DQA.Descriptors.Numeric, function(x) format(round((sum(na.omit(x) == FirstModes(x)[1])/sum(na.omit(x) == SecondModes(x)[1])),3), nsmall=3)),
  Minimum=sapply(DQA.Descriptors.Numeric, function(x) format(round(min(x,na.rm = TRUE),3), nsmall=3)),
  Mean=sapply(DQA.Descriptors.Numeric, function(x) format(round(mean(x,na.rm = TRUE),3), nsmall=3)),
  Median=sapply(DQA.Descriptors.Numeric, function(x) format(round(median(x,na.rm = TRUE),3), nsmall=3)),
  Maximum=sapply(DQA.Descriptors.Numeric, function(x) format(round(max(x,na.rm = TRUE),3), nsmall=3)),
  Skewness=sapply(DQA.Descriptors.Numeric, function(x) format(round(skewness(x,na.rm = TRUE),3), nsmall=3)),
  Kurtosis=sapply(DQA.Descriptors.Numeric, function(x) format(round(kurtosis(x,na.rm = TRUE),3), nsmall=3)),
  Percentile25th=sapply(DQA.Descriptors.Numeric, function(x) format(round(quantile(x,probs=0.25,na.rm = TRUE),3), nsmall=3)),
  Percentile75th=sapply(DQA.Descriptors.Numeric, function(x) format(round(quantile(x,probs=0.75,na.rm = TRUE),3), nsmall=3)),
  row.names=NULL)
  )

}

##################################
# Identifying potential data quality issues
##################################

##################################
# Checking for missing observations
##################################
if ((nrow(DQA.Summary[DQA.Summary$NA.Count>0,]))>0){
  print(paste0("Missing observations noted for ",
               (nrow(DQA.Summary[DQA.Summary$NA.Count>0,])),
               " variable(s) with NA.Count>0 and Fill.Rate<1.0."))
  DQA.Summary[DQA.Summary$NA.Count>0,]
} else {
  print("No missing observations noted.")
}

##################################
# Checking for zero or near-zero variance Descriptors
##################################
if (length(names(DQA.Descriptors.Factor))==0) {
  print("No factor descriptors noted.")
} else if (nrow(DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,])),
               " factor variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Descriptors.Factor.Summary[as.numeric(as.character(DQA.Descriptors.Factor.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance factor descriptors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,])),
               " numeric variable(s) with First.Second.Mode.Ratio>5."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$First.Second.Mode.Ratio))>5,]
} else {
  print("No low variance numeric descriptors due to high first-second mode ratio noted.")
}

if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,])>0){
  print(paste0("Low variance observed for ",
               (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,])),
               " numeric variable(s) with Unique.Count.Ratio<0.01."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Unique.Count.Ratio))<0.01,]
} else {
  print("No low variance numeric descriptors due to low unique count ratio noted.")
}

##################################
# Checking for skewed Descriptors
##################################
if (length(names(DQA.Descriptors.Numeric))==0) {
  print("No numeric descriptors noted.")
} else if (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),])>0){
  print(paste0("High skewness observed for ",
  (nrow(DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                               as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),])),
  " numeric variable(s) with Skewness>3 or Skewness<(-3)."))
  DQA.Descriptors.Numeric.Summary[as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))>3 |
                                 as.numeric(as.character(DQA.Descriptors.Numeric.Summary$Skewness))<(-3),]
} else {
  print("No skewed numeric descriptors noted.")
}

```

</details>

###  1.3.3 Data Preprocessing
|
|

####  1.3.3.1 Missing Data Imputation
|
| **[A]** Missing data identified for 1 out of the 30 descriptors were replaced using multivariate imputation by chained equations with predictive mean matching specified as the imputation method.
|      **[A.1]** <span style="color: #FF0000">ASMORT</span>: Null.Count=0, Fill.Rate=1.00
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.1, warning=FALSE, message=FALSE}
##################################
# Visualizing the missing data patterns
# prior to imputation
##################################
summary(DQA.Descriptors.Numeric)
visdat::vis_miss(DQA.Descriptors.Numeric, sort_miss = FALSE)

##################################
# Conducting missing data imputation
# using Multivariate Imputation by Chained Equations
##################################
MICE_Model <- mice(DQA.Descriptors.Numeric, method='pmm', seed = 123)
DQA.Descriptors.Numeric <- complete(MICE_Model)

##################################
# Visualizing the missing data patterns
# after imputation
##################################
visdat::vis_miss(DQA.Descriptors.Numeric, sort_miss = FALSE)

```
</details>

|
|
####  1.3.3.2 Outlier Detection
|
| **[A]** Minimal outliers noted for 10 out of the 30 descriptors. Descriptor values were visualized through a boxplot including observations classified as suspected outliers using the IQR criterion. The IQR criterion means that all observations above the (75th percentile + 1.5 x IQR) or below the (25th percentile - 1.5 x IQR) are suspected outliers, where IQR is the difference between the third quartile (75th percentile) and first quartile (25th percentile).
|      **[A.1]** <span style="color: #FF0000">LDMORT</span> = 1
|      **[A.2]** <span style="color: #FF0000">ARTINC</span> = 4
|      **[A.3]** <span style="color: #FF0000">RDMORT</span> = 2
|      **[A.4]** <span style="color: #FF0000">KDPREV</span> = 1
|      **[A.5]** <span style="color: #FF0000">PDMORT</span> = 1
|      **[A.6]** <span style="color: #FF0000">PDPREV</span> = 2
|      **[A.7]** <span style="color: #FF0000">DBMORT</span> = 1
|      **[A.8]** <span style="color: #FF0000">TTHLOS</span> = 2
|      **[A.9]** <span style="color: #FF0000">HLTINS</span> = 1
|      **[A.10]** <span style="color: #FF0000">SMPREV</span> = 1
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.2.1, warning=FALSE, message=FALSE}
##################################
# Loading dataset
##################################
DPA <- DQA.Descriptors.Numeric

##################################
# Gathering descriptive statistics
##################################
(DPA_Skimmed <- skim(DPA))
```

```{r section_1.3.3.2.2, warning=FALSE, message=FALSE, fig.width=15, fig.height=2}
##################################
# Outlier Detection
##################################

##################################
# Listing all Descriptors
##################################
DPA.Descriptors <- DPA

##################################
# Listing all numeric Descriptors
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors[,sapply(DPA.Descriptors, is.numeric)]

##################################
# Identifying outliers for the numeric Descriptors
##################################
OutlierCountList <- c()

for (i in 1:ncol(DPA.Descriptors.Numeric)) {
  Outliers <- boxplot.stats(DPA.Descriptors.Numeric[,i])$out
  OutlierCount <- length(Outliers)
  OutlierCountList <- append(OutlierCountList,OutlierCount)
  OutlierIndices <- which(DPA.Descriptors.Numeric[,i] %in% c(Outliers))
  print(
  ggplot(DPA.Descriptors.Numeric, aes(x=DPA.Descriptors.Numeric[,i])) +
  geom_boxplot() +
  theme_bw() +
  theme(axis.text.y=element_blank(), 
        axis.ticks.y=element_blank()) +
  xlab(names(DPA.Descriptors.Numeric)[i]) +
  labs(title=names(DPA.Descriptors.Numeric)[i],
       subtitle=paste0(OutlierCount, " Outlier(s) Detected")))
}

```

</details>

|
|
####  1.3.3.3 Zero and Near-Zero Variance
|
| **[A]** No low variance observed for any descriptor in the dataset.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.3, warning=FALSE, message=FALSE}
##################################
# Zero and Near-Zero Variance
##################################

##################################
# Identifying columns with low variance
###################################
DPA_LowVariance <- nearZeroVar(DPA,
                               freqCut = 80/20,
                               uniqueCut = 10,
                               saveMetrics= TRUE)
(DPA_LowVariance[DPA_LowVariance$nzv,])

if ((nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))==0){
  
  print("No low variance descriptors noted.")
  
} else {

  print(paste0("Low variance observed for ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s) with First.Second.Mode.Ratio>4 and Unique.Count.Ratio<0.10."))
  
  DPA_LowVarianceForRemoval <- (nrow(DPA_LowVariance[DPA_LowVariance$nzv,]))
  
  print(paste0("Low variance can be resolved by removing ",
               (nrow(DPA_LowVariance[DPA_LowVariance$nzv,])),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LowVarianceForRemoval) {
  DPA_LowVarianceRemovedVariable <- rownames(DPA_LowVariance[DPA_LowVariance$nzv,])[j]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LowVarianceRemovedVariable))
  }
  
  DPA %>%
  skim() %>%
  dplyr::filter(skim_variable %in% rownames(DPA_LowVariance[DPA_LowVariance$nzv,]))

}
```

</details>

|
|
####  1.3.3.4 Collinearity
|
| **[A]** High multicollinearity with Pearson correlation coefficients >90% was noted among pairs of descriptors in the dataset.
|      **[A.1]** <span style="color: #FF0000">HDMORT</span> and <span style="color: #FF0000">CDMORT</span> = 0.99
|      **[A.2]** <span style="color: #FF0000">DBPREV</span> and <span style="color: #FF0000">HLSTAT</span> = 0.91
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.4, warning=FALSE, message=FALSE}
##################################
# Measuring pairwise correlation between descriptors
##################################
(DPA_Correlation <- cor(DPA.Descriptors.Numeric,
                        method = "pearson",
                        use="pairwise.complete.obs"))

##################################
# Testing pairwise correlation between descriptors
##################################
DPA_CorrelationTest <- cor.mtest(DPA.Descriptors.Numeric,
                       method = "pearson",
                       conf.level = 0.95)

##################################
# Visualizing pairwise correlation between descriptors
##################################
corrplot(cor(DPA.Descriptors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"),
             method = "circle",
             type = "upper",
             order = "original",
             tl.col = "black",
             tl.cex = 0.75,
             tl.srt = 90,
             sig.level = 0.05,
             p.mat = DPA_CorrelationTest$p,
             insig = "blank")

corrplot(cor(DPA.Descriptors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"),
             method = "number",
             type = "upper",
             order = "original",
             tl.col = "black",
             tl.cex = 0.75,
             tl.srt = 90,
             sig.level = 0.05,
             p.mat = DPA_CorrelationTest$p,
             insig = "blank")

##################################
# Identifying the highly correlated variables
##################################
(DPA_HighlyCorrelatedCount <- sum(abs(DPA_Correlation[upper.tri(DPA_Correlation)])>0.90))

if (DPA_HighlyCorrelatedCount > 0) {
  DPA_HighlyCorrelated <- findCorrelation(DPA_Correlation, cutoff = 0.90)
  
  (DPA_HighlyCorrelatedForRemoval <- length(DPA_HighlyCorrelated))
  
  print(paste0("High correlation can be resolved by removing ",
               (DPA_HighlyCorrelatedForRemoval),
               " numeric variable(s)."))
  
  for (j in 1:DPA_HighlyCorrelatedForRemoval) {
    DPA_HighlyCorrelatedRemovedVariable <- colnames(DPA.Descriptors.Numeric)[DPA_HighlyCorrelated[j]]
    print(paste0("Variable ",
                 j,
                 " for removal: ",
                 DPA_HighlyCorrelatedRemovedVariable))
  }
  
}

if (DPA_HighlyCorrelatedCount == 0) {
  print("No highly correlated predictors noted.")
} else {
  print(paste0("High correlation observed for ",
               (DPA_HighlyCorrelatedCount),
               " pairs of numeric variable(s) with Correlation.Coefficient>0.90."))
  
  (DPA_HighlyCorrelatedPairs <- corr_cross(DPA.Descriptors.Numeric,
  max_pvalue = 0.05, 
  top = DPA_HighlyCorrelatedCount,
  rm.na = TRUE,
  grid = FALSE
))
  
}

```

</details>

|
|
####  1.3.3.5 Linear Dependencies
|
| **[A]** No linear dependencies observed for any subset of decriptors in the dataset. 
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.5, warning=FALSE, message=FALSE}
##################################
# Linear Dependencies
##################################

##################################
# Finding linear dependencies
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Descriptors.Numeric)

##################################
# Identifying the linearly dependent variables
##################################
DPA_LinearlyDependent <- findLinearCombos(DPA.Descriptors.Numeric)

(DPA_LinearlyDependentCount <- length(DPA_LinearlyDependent$linearCombos))

if (DPA_LinearlyDependentCount == 0) {
  print("No linearly dependent predictors noted.")
} else {
  print(paste0("Linear dependency observed for ",
               (DPA_LinearlyDependentCount),
               " subset(s) of numeric variable(s)."))
  
  for (i in 1:DPA_LinearlyDependentCount) {
    DPA_LinearlyDependentSubset <- colnames(DPA.Descriptors.Numeric)[DPA_LinearlyDependent$linearCombos[[i]]]
    print(paste0("Linear dependent variable(s) for subset ",
                 i,
                 " include: ",
                 DPA_LinearlyDependentSubset))
  }
  
}

##################################
# Identifying the linearly dependent variables for removal
##################################

if (DPA_LinearlyDependentCount > 0) {
  DPA_LinearlyDependent <- findLinearCombos(DPA.Descriptors.Numeric)
  
  DPA_LinearlyDependentForRemoval <- length(DPA_LinearlyDependent$remove)
  
  print(paste0("Linear dependency can be resolved by removing ",
               (DPA_LinearlyDependentForRemoval),
               " numeric variable(s)."))
  
  for (j in 1:DPA_LinearlyDependentForRemoval) {
  DPA_LinearlyDependentRemovedVariable <- colnames(DPA.Descriptors.Numeric)[DPA_LinearlyDependent$remove[j]]
  print(paste0("Variable ",
               j,
               " for removal: ",
               DPA_LinearlyDependentRemovedVariable))
  }

}

```

</details>

|
|
####  1.3.3.6 Distributional Shape
|
| **[A]** No shape transformation was necessary as the distributional skewness observed among individual descriptors was normal (all values within +3 and -3) with minimal outliers.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.3.6, warning=FALSE, message=FALSE, fig.width=15, fig.height=2}
##################################
# Distributional Shape
##################################

##################################
# Formulating the histogram
# for the numeric descriptors
##################################
for (i in 1:ncol(DPA.Descriptors.Numeric)) {
  Median <- format(round(median(DPA.Descriptors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  Mean <- format(round(mean(DPA.Descriptors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  Skewness <- format(round(skewness(DPA.Descriptors.Numeric[,i],na.rm = TRUE),2), nsmall=2)
  print(
  ggplot(DPA.Descriptors.Numeric, aes(x=DPA.Descriptors.Numeric[,i])) +
  geom_histogram(binwidth=1,color="black", fill="white") +
  geom_vline(aes(xintercept=mean(DPA.Descriptors.Numeric[,i])),
            color="blue", size=1) +
    geom_vline(aes(xintercept=median(DPA.Descriptors.Numeric[,i])),
            color="red", size=1) +
  theme_bw() +
  ylab("Count") +
  xlab(names(DPA.Descriptors.Numeric)[i]) +
  labs(title=names(DPA.Descriptors.Numeric)[i],
       subtitle=paste0("Median = ", Median,
                       ", Mean = ", Mean,
                       ", Skewness = ", Skewness)))
}

```

</details>

###  1.3.4 Data Pre-Assessment
|
|
####  1.3.4.1 Correlation Matrix Assessment - Kaiser-Meyer-Olkin Factor Adequacy
|
| **[A]** The KMO measure of sampling adequacy was acceptable with a computed value of 0.85 for the complete model, indicating the suitability of the data for exploratory factor analysis. The estimated proportion of variance among all the observed variable was sufficiently adequate.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.4.1, warning=FALSE, message=FALSE}
##################################
# Removing individual descriptors
# among the highly correlated pair
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("CDMORT","HLSTAT")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Removing individual descriptors
# with low KMO values
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("ASMORT")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Removing individual descriptors
# with low KMO values
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("PNEVAC")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Removing individual descriptors
# with low KMO values
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("INFVAC")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Removing individual descriptors
# with low KMO values
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("ASPREV")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Removing individual descriptors
# with low KMO values
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("MAMUSE")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Removing individual descriptors
# with low KMO values
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("SLPSTA")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Removing individual descriptors
# with low KMO values
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("KDPREV")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Removing individual descriptors
# with low KMO values
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("QUITAT")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Removing individual descriptors
# with low KMO values
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("LDMORT")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Removing individual descriptors
# with low KMO values
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("SCPREV")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Removing individual descriptors
# with low KMO values
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("PSMUSE")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

##################################
# Removing individual descriptors
# with low KMO values
##################################
DPA.Descriptors.Numeric <- DPA.Descriptors.Numeric[,!colnames(DPA.Descriptors.Numeric) %in% c("HLTINS")]

##################################
# Calculating the Kaiser-Meyer-Olkin Factor Adequacy
##################################
(DPA_KMOFactorAdequacy <- KMO(DPA.Descriptors.Numeric))

```

</details>

|
|
####  1.3.4.2 Correlation Matrix Assessment - Bartlett’s Test of Sphericity
|
| **[A]** The computed p-value from the Bartlett’s Test of Sphericity was statistically significant (<0.00001), rejecting the null hypothesis that the correlation matrix is an identity matrix (ones on the diagonal and zeros on the off-diagonal). Results indicated that there is enough evidence to support the existence of underlying factors, suggesting that the correlation matrix is appropriate for exploratory factor analysis.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.4.2, warning=FALSE, message=FALSE}
##################################
# Calculating the Bartlett's Test of Sphericity
##################################
(DPA_BartlettTest <- cortest.bartlett(DPA.Descriptors.Numeric,
                                      n=nrow(DPA.Descriptors.Numeric)))

```

</details>

|
|
####  1.3.4.3 Correlation Matrix Assessment - Covariance Validity
|
| **[A]** Covariance among descriptors in the correlation matrix was sufficient to justify the conduct of an exploratory factor analysis. 91% (124/136) of the pairwise associations using the Pearson correlation coefficient were above 30%.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.4.3, warning=FALSE, message=FALSE}
##################################
# Measuring pairwise correlation between descriptors
##################################
(DPA_Correlation <- cor(DPA.Descriptors.Numeric,
                        method = "pearson",
                        use="pairwise.complete.obs"))

##################################
# Identifying the minimally correlated variables
##################################
(DPA_MinimallyCorrelatedCount <- sum(abs(DPA_Correlation[upper.tri(DPA_Correlation)])>0.30))

(DPA_AllPairs <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)

(DPA_MinimallyCorrelatedCountPercentage <- DPA_MinimallyCorrelatedCount/DPA_AllPairs)

qgraph(cor(DPA.Descriptors.Numeric),
       cut=0.30,
       details=FALSE,
       posCol="#2F75B5",
       negCol="#FF5050",
       labels=names(DPA.Descriptors.Numeric))

```

</details>
|
|
####  1.3.4.4 Correlation Matrix Assessment - Determinant Computation
|
| **[A]** The determinant of the correlation matrix was computed as less than .00001 indicating the minimal presence of the likelihood for a multicollinearity problem allowing matrix operations to potentially produce unstable results during exploratory factor analysis.
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.4.4, warning=FALSE, message=FALSE}
##################################
# Computing the determinant of the correlation matrix
##################################
(DPA_CorrelationMatrixDeterminant <- det(cor(DPA.Descriptors.Numeric)))

```

</details>

|
|
####  1.3.4.5 Pre-Processed Dataset
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.4.5, warning=FALSE, message=FALSE}

```

</details>

|
|
###  1.3.5 Data Exploration
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.5, warning=FALSE, message=FALSE}

```

</details>

|
|
####  1.3.5.1 Correlation Matrix
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.5.1, warning=FALSE, message=FALSE}
##################################
# Testing pairwise correlation between descriptors
##################################
DPA_CorrelationTest <- cor.mtest(DPA.Descriptors.Numeric,
                       method = "pearson",
                       conf.level = 0.95)

##################################
# Plotting the pairwise association
# between descriptors 
##################################
corrplot(cor(DPA.Descriptors.Numeric,
             method = "pearson",
             use="pairwise.complete.obs"),
             method = "number",
             type = "upper",
             order = "original",
             tl.col = "black",
             tl.cex = 0.75,
             tl.srt = 90,
             sig.level = 0.05,
             p.mat = DPA_CorrelationTest$p,
             insig = "blank")
```

</details>

|
|
####  1.3.5.2 Scatterplot Matrix
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.5.2, warning=FALSE, message=FALSE}
##################################
# Formulating the pairwise scatterplots
# among descriptors
##################################
splom(~DPA.Descriptors.Numeric,
      pch = 16,
      cex = 1,
      alpha = 0.45,
      auto.key = list(points = TRUE, space = "right"),
      main = "Exploratory Analysis Between Descriptors",
      xlab = "Scatterplot Matrix of Descriptors")
```

</details>

|
|
###  1.3.6 Model Development
|
|
####  1.3.6.1 Principal Axes Factor Extraction and Varimax Rotation (FA_PA_V)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.6.1, warning=FALSE, message=FALSE}
##################################
# Implementing various procedures for determining
# factor retention based on
# the maximum consensus between methods
##################################
(FA_PA_V_MethodAgreementProcedure <- parameters::n_factors(DPA.Descriptors.Numeric,
                                                           algorithm = "pa",
                                                           rotation = "varimax"))

as.data.frame(FA_PA_V_MethodAgreementProcedure)

##################################
# Conducting exploratory factor analysis
# using Principal Axes extraction
# and Varimax rotation
# with 3 factors
##################################
(FA_PA_V_3F <- fa(DPA.Descriptors.Numeric,
              nfactors = 3,
              fm="pa",
              rotate = "varimax",
              residuals=TRUE,
              SMC=TRUE,
              n.obs=nrow(DPA.Descriptors.Numeric)))

(FA_PA_V_3F_Summary <- FA_PA_V_3F %>%
  model_parameters(sort = TRUE, threshold = "max"))

summary(FA_PA_V_3F_Summary)

##################################
# Extracting the residuals
# from the Exploratory Factor Analysis
##################################
(FA_PA_V_3F_Residual <- residuals(FA_PA_V_3F,
                              diag=FALSE,
                              na.rm=TRUE))

##################################
# Obtaining Fit Indices
##################################
(FA_PA_V_3F_RMS <- FA_PA_V_3F$rms)

(FA_PA_V_3F_TLI <- FA_PA_V_3F$TLI)

(FA_PA_V_3F_BIC <- FA_PA_V_3F$BIC)

(FA_PA_V_3F_MaxResidual   <- max(abs(FA_PA_V_3F_Residual),na.rm=TRUE))

(FA_PA_V_3F_HighResidual  <- sum(FA_PA_V_3F_Residual>abs(0.05),na.rm=TRUE))

(FA_PA_V_3F_TotalResidual <- length(DPA.Descriptors.Numeric)*(length(DPA.Descriptors.Numeric)-1)/2)

(FA_PA_V_3F_HighResidualRate <- FA_PA_V_3F_HighResidual/FA_PA_V_3F_TotalResidual)

##################################
# Graph the factor loading matrices
##################################
fa.diagram(FA_PA_V_3F,
           sort=TRUE,
           cut=0,
           digits=3,
           main="Principal Axes Factor Extraction + Varimax Rotation : 3 Factors",
           cex=0.75)
```

</details>

|
|
####  1.3.6.2 Principal Axes Factor Extraction and Promax Rotation (FA_PA_P)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.6.2, warning=FALSE, message=FALSE}

```

</details>

|
|
####  1.3.6.3 Maximum Likelihood Factor Extraction and Varimax Rotation (FA_ML_V)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.6.3, warning=FALSE, message=FALSE}

```

</details>

|
|
####  1.3.6.4 Maximum Likelihood Factor Extraction and Promax Rotation (FA_ML_P)
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.6.4, warning=FALSE, message=FALSE}

```

</details>

|
|
###  1.3.7 Model Performance Validation
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.7, warning=FALSE, message=FALSE}

```

</details>

###  1.3.8 Model Selection
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.8, warning=FALSE, message=FALSE}

```

</details>

###  1.3.9 Model Presentation
|
|
####  1.3.9.1 Exploratory Factor Analysis Dandelion Plot
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.9.1, warning=FALSE, message=FALSE}

```

</details>

|
|
####  1.3.9.2 Confirmatory Factor Analysis Path Diagram
|
|

<details><summary><mark style="background-color: #000000;color: #FFFFFF">**Code Chunk | Output**</mark></summary>

```{r section_1.3.9.2, warning=FALSE, message=FALSE}

```

</details>
|
|
# **2. Summary** <a name="summary"></a>
|
| **Details.**
|
| **[A]** Details
|
|
# **3. References**
|
| **[Book]** [Using Multivariate Analysis](https://www.pearson.com/en-us/subject-catalog/p/using-multivariate-statistics/P200000003097/9780137526543) by Barbara Tabachnick and Linda Fidell
| **[Book]** [A Step-by-Step Guide to Exploratory Factor Analysis with R and RStudio](https://www.taylorfrancis.com/books/mono/10.4324/9781003120001/step-step-guide-exploratory-factor-analysis-rstudio-marley-watkins) by Marley Watkins
| **[Book]** [Just Enough R](https://benwhalley.github.io/just-enough-r/) by Ben Whalley
| **[Book]** [Multiple Factor Analysis by Example Using R](https://www.oreilly.com/library/view/multiple-factor-analysis/9781498786690/) by Jerome Pages
| **[Book]** [Nonlinear Principal Component Analysis and Its Applications](https://link.springer.com/book/10.1007/978-981-10-0159-8#toc) by Yuichi Mori, Masahiro Kuroda and Naomichi Makino
| **[Book]** [Applied Predictive Modeling](http://appliedpredictivemodeling.com/) by Max Kuhn and Kjell Johnson
| **[Book]** [An Introduction to Statistical Learning](https://www.statlearning.com/) by Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani
| **[Book]** [Multivariate Data Visualization with R](http://lmdvr.r-forge.r-project.org/figures/figures.html) by Deepayan Sarkar
| **[Book]** [Machine Learning](https://bookdown.org/ssjackson300/Machine-Learning-Lecture-Notes/) by Samuel Jackson
| **[Book]** [Data Modeling Methods](https://bookdown.org/larget_jacob/data-modeling-methods/) by Jacob Larget
| **[Book]** [Introduction to R and Statistics](https://saestatsteaching.tech/) by University of Western Australia
| **[Book]** [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/index.html) by Max Kuhn and Kjell Johnson
| **[Book]** [Introduction to Research Methods](https://bookdown.org/ejvanholm/Textbook/) by Eric van Holm
| **[Book]** [Using R for Social Work Research](https://bookdown.org/bean_jerry/using_r_for_social_work_research/) by Jerry Bean
| **[Book]** [Introduction to R](https://methodenlehre.github.io/SGSCLM-R-course/index.html#requirements) by Andrew Ellis and Boris Mayer
| **[R Package]** [AppliedPredictiveModeling](https://cran.r-project.org/web//packages/AppliedPredictiveModeling/AppliedPredictiveModeling.pdf) by Max Kuhn
| **[R Package]** [caret](https://topepo.github.io/caret/index.html) by Max Kuhn
| **[R Package]** [rpart](https://mran.microsoft.com/web/packages/rpart/rpart.pdf) by Terry Therneau and Beth Atkinson
| **[R Package]** [lattice](https://cran.r-project.org/web/packages/lattice/lattice.pdf) by  Deepayan Sarkar
| **[R Package]** [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html/) by Hadley Wickham
| **[R Package]** [tidyr](https://cran.r-project.org/web/packages/tidyr/tidyr.pdf) by Hadley Wickham
| **[R Package]** [moments](https://cran.r-project.org/web/packages/moments/index.html) by Lukasz Komsta and Frederick
| **[R Package]** [skimr](https://cran.r-project.org/web/packages/skimr/skimr.pdf) by  Elin Waring
| **[R Package]** [RANN](https://cran.r-project.org/web/packages/RANN/RANN.pdf) by  Sunil Arya, David Mount, Samuel Kemp and Gregory Jefferis
| **[R Package]** [corrplot](https://cran.r-project.org/web/packages/corrplot/corrplot.pdf) by Taiyun Wei
| **[R Package]** [tidyverse](https://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf) by Hadley Wickham
| **[R Package]** [lares](https://cran.rstudio.com/web/packages/lares/lares.pdf) by Bernardo Lares
| **[R Package]** [DMwR](https://mran.microsoft.com/snapshot/2016-05-02/web/packages/DMwR/DMwR.pdf) by Luis Torgo
| **[R Package]** [gridExtra](https://cran.r-project.org/web/packages/gridExtra/gridExtra.pdf) by Baptiste Auguie and Anton Antonov
| **[R Package]** [rattle](https://cran.r-project.org/web/packages/rattle/rattle.pdf) by Graham Williams
| **[R Package]** [RColorBrewer](https://cran.r-project.org/web//packages/RColorBrewer/RColorBrewer.pdf) by Erich Neuwirth
| **[R Package]** [stats](https://search.r-project.org/R/refmans/stats/html/00Index.html) by R Core Team
| **[R Package]** [factoextra](https://cran.r-project.org/web/packages/factoextra/factoextra.pdf) by Alboukadel Kassambara and Fabian Mundt
| **[R Package]** [FactoMineR](https://search.r-project.org/R/refmans/stats/html/00Index.html) by Francois Husson, Julie Josse, Sebastien Le and Jeremy Mazet
| **[R Package]** [gplots](https://cran.r-project.org/web/packages/gplots/gplots.pdf) by Tal Galili
| **[R Package]** [qgraph](https://cran.r-project.org/web/packages/qgraph/qgraph.pdf) by Sacha Epskamp
| **[R Package]** [ggplot2](https://search.r-project.org/R/refmans/stats/html/00Index.html) by Hadley Wickham, Winston Chang, Lionel Henry and Thomas Lin Pedersen
| **[R Package]** [psych](https://cran.r-project.org/web/packages/psych/psych.pdf) by William Revelle
| **[R Package]** [nFactors](https://cran.r-project.org/web/packages/nFactors/nFactors.pdf) by Gilles Raiche and David Magis
| **[R Package]** [MBESS](https://cran.r-project.org/web/packages/MBESS/MBESS.pdf) by Ken Kelley
| **[R Package]** [DandEFA](https://cran.r-project.org/web/packages/DandEFA/DandEFA.pdf) by Artur Manukyan, Ahmet Sedef, Erhan Cene and Ibrahim Demir
| **[R Package]** [EFAtools](https://cran.r-project.org/web/packages/EFAtools/EFAtools.pdf) by Markus Steiner and Silvia Grieder
| **[R Package]** [parameters](https://cran.r-project.org/web/packages/parameters/parameters.pdf) by Daniel Ludecke
| **[R Package]** [performance](https://cran.r-project.org/web/packages/performance/performance.pdf) by Daniel Ludecke
| **[R Package]** [HH](https://cran.r-project.org/web/packages/HH/HH.pdf) by Richard Heiberger
| **[Article]** [6 Dimensionality Reduction Techniques in R (with Examples)](https://cmdlinetips.com/2022/07/dimensionality-reduction-techniques-in-r/) by CMDLineTips Team
| **[Article]** [6 Dimensionality Reduction Algorithms With Python](https://machinelearningmastery.com/dimensionality-reduction-algorithms-with-python/) by Jason Brownlee
| **[Article]** [Introduction to Dimensionality Reduction for Machine Learning](https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/) by Jason Brownlee
| **[Article]** [Introduction to Dimensionality Reduction](https://www.geeksforgeeks.org/dimensionality-reduction/) by Geeks For Geeks
| **[Article]** [Factor Analysis with the psych package](https://m-clark.github.io/posts/2020-04-10-psych-explained/) by Michael Clark
| **[Article]** [Factor Analysis in R with Psych Package: Measuring Consumer Involvement](https://www.r-bloggers.com/2019/01/factor-analysis-in-r-with-psych-package-measuring-consumer-involvement/) by Peter Prevos
| **[Article]** [Factor Analysis in R](http://jinjian-mu.com/tutorial/2021-04-14-Factor%20Analysis/) by Jinjian Mu
| **[Article]** [How To: Use the psych package for Factor Analysis and Data Reduction](http://personality-project.org/r/psych/HowTo/factor.pdf) by William Revelle
| **[Article]** [A Practical Introduction to Factor Analysis: Exploratory Factor Analysis](https://stats.oarc.ucla.edu/spss/seminars/introduction-to-factor-analysis/a-practical-introduction-to-factor-analysis/) by UCLA Advanced Research Computing Team
| **[Article]** [Examining the Big 5 Personality Dataset with Factor Analysis](https://taridwong.github.io/posts/2022-01-01-efacfa/) by Tarid Wongvorachan
| **[Article]** [Principal Component Analysis versus Exploratory Factor Analysis](https://support.sas.com/resources/papers/proceedings/proceedings/sugi30/203-30.pdf) by Diana Suhr
| **[Article]** [Exploratory Factor Analysis](https://www.publichealth.columbia.edu/research/population-health-methods/exploratory-factor-analysis) by Columbia University Irving Medical Center
| **[Article]** [Factor Analysis Example](https://real-statistics.com/multivariate-statistics/factor-analysis/factor-analysis-example/) by Charles Zaiontz
| **[Article]** [Factor Analysis Guide with an Example](https://statisticsbyjim.com/basics/factor-analysis/) by Jim Frost
| **[Article]** [What Is Factor Analysis and How Does It Simplify Research Findings?](https://www.qualtrics.com/experience-management/research/factor-analysis/) by Qualtrics Team
| **[Article]** [How Can I Perform A Factor Analysis With Categorical (Or Categorical And Continuous) Variables?](https://stats.oarc.ucla.edu/stata/faq/how-can-i-perform-a-factor-analysis-with-categorical-or-categorical-and-continuous-variables/) by UCLA Advanced Research Computing Team
| **[Article]** [Factor Analysis on Ordinal Data Example in R (psych, homals)](https://alice86.github.io/2018/04/08/Factor-Analysis-on-Ordinal-Data-example-in-R-(psych,-homals)/) by Jiayu Wu
| **[Article]** [Factor Analysis](https://handwiki.org/wiki/Factor%20analysis) by HandWiki Team
| **[Article]** [On Likert Scales In R](https://jakec007.github.io/2021-06-23-R-likert/) by Jake Chanenson
| **[Article]** [Confirmatory Factor Analysis (CFA) in R With LAVAAN](https://stats.oarc.ucla.edu/r/seminars/rcfa/) by UCLA Advanced Research Computing Team
| **[Article]** [Confirmatory Factor Analysis (CFA) Example in R](https://pj.freefaculty.org/guides/crmda_workshops/semexample/R/Ex-02-CFA/cfa-01.html) by Ben Kite
| **[Article]** [Confirmatory Factor Analysis with R](https://shiny.rit.albany.edu/stat/cfa1test/cfabase.pdf) by Bruce Dudek
| **[Article]** [A CFA Example](https://lavaan.ugent.be/tutorial/cfa.html) by Lavaan.Org
| **[RPubs Project]** [EFA and CFA Intro](https://rpubs.com/isbell_daniel/efa_cfa_intro) by Dan Isbell
| **[RPubs Project]** [Factor Analysis: CFA](https://rpubs.com/scblanco/944896) by Silvia Blanco
| **[RPubs Project]** [Exploratory and Confirmatory Factor Analysis in R](https://rpubs.com/Symrna/EFA_CFA) by Ergul Demir
| **[RPubs Project]** [Confirmatory Factor Analysis - CFA](https://rpubs.com/mikhilesh/643303) by Mikhilesh Dehane
| **[RPubs Project]** [Exploratory and Confirmatory Factor Analysis on SF-36 Scales](https://rpubs.com/jbeckste/FA_demo) by Jason Beckstead
| **[RPubs Project]** [Using R for Social Work Research: Exploratory Factor Analysis](https://rpubs.com/JBean/840128) by Jerry Bean
| **[RPubs Project]** [Using R for Social Work Research: Confirmatory Factor Analysis](https://rpubs.com/JBean/723517) by Jerry Bean
| **[Publication]** [General Intelligence Objectively Determined and Measured](https://psycnet.apa.org/record/1926-00296-001) by Charles Spearman (The American Journal of Psychology)
| **[Publication]** [The Effect of Standardization on a Chi-Square Approximation in Factor Analysis](https://www.semanticscholar.org/paper/THE-EFFECT-OF-STANDARDIZATION-ON-A-%CF%872-APPROXIMATION-Bartlett/95d549d2c055360b34cc7d1fce739179c29e39bb) by Maurice Bartlett (Psychometrika)
| **[Publication]** [A Second Generation Little Jiffy](https://link.springer.com/article/10.1007/BF02291817) by Henry Kaiser (Psychometrika)
| **[Publication]** [Tests of Significance in Factor Analysis](https://www.semanticscholar.org/paper/TESTS-OF-SIGNIFICANCE-IN-FACTOR-ANALYSIS-Burt/25975f19d17ab2577845ec3d61f52806b28d8f28) by Maurice Bartlett (British Journal of Statistical Psychology)
| **[Publication]** [Test of Linear Trend in Eigenvalues of a Covariance Matrix with Application to Data Analysis](https://psycnet.apa.org/record/1996-01978-006) by Peter Bentler and KeHai Yuan (British Journal of Mathematical and Statistical Psychology)
| **[Publication]** [The Scree Test For The Number Of Factors](https://www.semanticscholar.org/paper/The-Scree-Test-For-The-Number-Of-Factors.-Cattell/379df72de684003963f11427c97490a8c2d2a593) by Raymond Cattell (Multivariate Behavioral Research)
| **[Publication]** [Using Fit Statistic Differences to Determine the Optimal Number of Factors to Retain in an Exploratory Factor Analysis](https://journals.sagepub.com/doi/10.1177/0013164419865769) by William Finch (Educational and Psychological Measurement)
| **[Publication]** [An Objective Counterpart to the Visual Scree Test for Factor Analysis: The Standard Error Scree](https://journals.sagepub.com/doi/10.1177/0013164496056003006) by Keith Zoski and Stephen Jurs (Educational and Psychological Measurement)
| **[Publication]** [The Performance of Regression-Based Variations of the Visual Scree for Determining the Number of Common Factors](https://journals.sagepub.com/doi/10.1177/00164402062003001) by Fadia Nasser, Jeri Benson and Joseph Wisenbaker (Educational and Psychological Measurement)
| **[Publication]** [Investigating the Performance of Exploratory Graph Analysis and Traditional Techniques to Identify the Number of Latent Factors: A Simulation and Tutorial](https://www.semanticscholar.org/paper/Investigating-the-performance-of-exploratory-graph-Golino-Shi/470c4e8aeebd08699fe9092463540a1b24b7e2e8) by Hudson Golino, Dingjing Shi, Alexander Christensen, Luis Garrido, Maria Nieto, Ritu Sadana, Jotheeswaran Thiyagarajan, Agustin Martinez-Molina (Psychological Methods)
| **[Publication]** [Exploratory Graph Analysis: A New Approach for Estimating the Number of Dimensions in Psychological Research](https://www.semanticscholar.org/paper/Exploratory-graph-analysis%3A-A-new-approach-for-the-Golino-Epskamp/f44110bff4345eb228b27de8a0b8aec235edd478) by Hudson Galino and Sacha Epskamp (Plos One)
| **[Publication]** [Very Simple Structure: An Alternative Procedure For Estimating The Optimal Number Of Interpretable Factors](https://www.tandfonline.com/doi/abs/10.1207/s15327906mbr1404_2) by William Revelle and Thomas Rocklin (Multivariate Behavioral Research )
| **[Publication]** [Determining the Number of Components from the Matrix of Partial Correlations](https://psycnet.apa.org/record/1977-07293-001) by Wayne Velicer (Psychometrika)
| **[Publication]** [Dandelion Plot: A Method for the Visualization of R-mode Exploratory Factor Analyses](https://link.springer.com/article/10.1007/s00180-014-0518-x) by Artur Manukyan, Erhan Cene, Ahmet Sedef and Ibrahim Demir (Computational Statistics) 
| **[Course]** [Applied Data Mining and Statistical Learning](https://online.stat.psu.edu/stat508/) by Penn State Eberly College of Science
|
|
|
|